{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb088d1",
   "metadata": {},
   "source": [
    "# BirdCLEF+ 2025 Competition: CPU-Optimized Ensemble Model\n",
    "\n",
    "This notebook implements a time-efficient ensemble approach for the BirdCLEF+ 2025 competition, designed to run within the 90-minute CPU runtime limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee52fe",
   "metadata": {},
   "source": [
    "## Competition Requirements\n",
    "\n",
    "- CPU Notebook â‰¤ 90 minutes run-time\n",
    "- GPU Notebook submissions are disabled (only 1 minute of runtime)\n",
    "- Internet access disabled\n",
    "- Freely & publicly available external data is allowed, including pre-trained models\n",
    "- Submission file must be named submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5617a",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries\n",
    "\n",
    "Import only the essential libraries to reduce startup time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ba914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# IMPORTANT: Prevent any internet access attempts in Kaggle environment\n",
    "os.environ['NO_PROXY'] = '*'\n",
    "\n",
    "# Suppress warnings to reduce output clutter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check for CPU/GPU availability - we're restricted to CPU for this competition\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Show pandas version for reference\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d6332",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "Efficiently load the dataset with minimal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6743bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to data based on Kaggle's file structure\n",
    "BASE_DIR = \"/kaggle/input/birdclef-2025\" if os.path.exists(\"/kaggle/input\") else \"../input/birdclef-2025\"\n",
    "TRAIN_AUDIO_DIR = os.path.join(BASE_DIR, \"train_audio\")\n",
    "TRAIN_SOUNDSCAPES_DIR = os.path.join(BASE_DIR, \"train_soundscapes\")\n",
    "TEST_SOUNDSCAPES_DIR = os.path.join(BASE_DIR, \"test_soundscapes\")\n",
    "\n",
    "# Check if we're running on Kaggle\n",
    "is_kaggle = os.path.exists(\"/kaggle/input\")\n",
    "print(f\"Running on Kaggle: {is_kaggle}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "# Load training metadata\n",
    "train_csv_path = os.path.join(BASE_DIR, \"train.csv\")\n",
    "if os.path.exists(train_csv_path):\n",
    "    print(f\"Loading training CSV file: {train_csv_path}\")\n",
    "    train_metadata = pd.read_csv(train_csv_path)\n",
    "else:\n",
    "    print(f\"Training CSV file not found at {train_csv_path}\")\n",
    "    # Create a minimal structure for testing\n",
    "    train_metadata = pd.DataFrame({\n",
    "        'primary_label': ['species1', 'species2'] * 5,\n",
    "        'filename': [f'dummy{i}.ogg' for i in range(10)],\n",
    "        'duration': [5.0] * 10\n",
    "    })\n",
    "\n",
    "print(f\"\\nDataset overview: {len(train_metadata)} samples, {train_metadata['primary_label'].nunique()} species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36492f",
   "metadata": {},
   "source": [
    "## 3. Efficient Audio Processing & Feature Extraction\n",
    "\n",
    "Streamlined preprocessing and feature extraction optimized for CPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254320ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing parameters - optimized for speed\n",
    "SAMPLE_RATE = 32000  # Common for bird sound analysis\n",
    "MAX_AUDIO_LENGTH = 5  # Maximum audio length in seconds \n",
    "AUDIO_LENGTH_SAMPLES = MAX_AUDIO_LENGTH * SAMPLE_RATE\n",
    "N_MELS = 128  # Number of MEL bands\n",
    "N_MFCC = 20  # Reduced number of MFCCs for efficiency\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 1024  # Reduced from 2048 for speed\n",
    "\n",
    "# Set up caching to avoid recomputing features\n",
    "CACHE_DIR = Path(\"./feature_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def load_audio_file(file_path, sr=SAMPLE_RATE, duration=None):\n",
    "    \"\"\"Load audio file with optional resampling and duration limit\"\"\"\n",
    "    try:\n",
    "        # Use mono=True to reduce computation\n",
    "        audio, _ = librosa.load(file_path, sr=sr, duration=duration, mono=True)\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def pad_or_trim(audio, target_length=AUDIO_LENGTH_SAMPLES):\n",
    "    \"\"\"Pad with zeros or trim audio to target length\"\"\"\n",
    "    if len(audio) < target_length:\n",
    "        return np.pad(audio, (0, target_length - len(audio)), 'constant')\n",
    "    else:\n",
    "        return audio[:target_length]\n",
    "\n",
    "def extract_features(audio, feature_type='mel'):\n",
    "    \"\"\"Extract either mel spectrograms or MFCCs\"\"\"\n",
    "    if feature_type == 'mel':\n",
    "        # Extract MEL spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, \n",
    "            sr=SAMPLE_RATE, \n",
    "            n_mels=N_MELS,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH\n",
    "        )\n",
    "        # Convert to decibels (log scale)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec_db\n",
    "    \n",
    "    elif feature_type == 'mfcc':\n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=audio, \n",
    "            sr=SAMPLE_RATE, \n",
    "            n_mfcc=N_MFCC,\n",
    "            n_fft=N_FFT, \n",
    "            hop_length=HOP_LENGTH\n",
    "        )\n",
    "        # Normalize\n",
    "        mfccs = (mfccs - np.mean(mfccs)) / (np.std(mfccs) + 1e-8)\n",
    "        return mfccs\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature type: {feature_type}\")\n",
    "\n",
    "def preprocess_and_extract(file_path, feature_type='mel', use_cache=True):\n",
    "    \"\"\"End-to-end preprocessing and feature extraction with caching\"\"\"\n",
    "    # Generate cache key\n",
    "    cache_key = hashlib.md5(f\"{file_path}_{feature_type}_{SAMPLE_RATE}_{N_MELS}_{N_FFT}\".encode()).hexdigest()\n",
    "    cache_path = CACHE_DIR / f\"{cache_key}.npy\"\n",
    "    \n",
    "    # Try to load from cache first\n",
    "    if use_cache and cache_path.exists():\n",
    "        try:\n",
    "            return np.load(cache_path)\n",
    "        except:\n",
    "            pass  # Fall back to recomputing if cache load fails\n",
    "            \n",
    "    # Process audio\n",
    "    audio = load_audio_file(file_path, duration=MAX_AUDIO_LENGTH)\n",
    "    if audio is None:\n",
    "        return None\n",
    "    \n",
    "    audio = pad_or_trim(audio)\n",
    "    features = extract_features(audio, feature_type)\n",
    "    \n",
    "    # Save to cache\n",
    "    if use_cache:\n",
    "        try:\n",
    "            np.save(cache_path, features)\n",
    "        except:\n",
    "            pass  # Continue even if cache saving fails\n",
    "            \n",
    "    return features\n",
    "\n",
    "# Function to batch process a subset of files\n",
    "def batch_process_files(file_list, max_files=100, feature_type='mel'):\n",
    "    \"\"\"Process a batch of files and return features and labels\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    \n",
    "    # Use a limited number of files to fit within time constraints\n",
    "    subset_files = file_list[:max_files]\n",
    "    \n",
    "    print(f\"Processing {len(subset_files)} audio files...\")\n",
    "    for idx, (filename, label) in enumerate(subset_files):\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Processing file {idx+1}/{len(subset_files)}\")\n",
    "            \n",
    "        # Find file path\n",
    "        file_path = os.path.join(TRAIN_AUDIO_DIR, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            # Try with species subfolder\n",
    "            file_path = os.path.join(TRAIN_AUDIO_DIR, label, filename)\n",
    "            if not os.path.exists(file_path):\n",
    "                continue\n",
    "        \n",
    "        # Extract features\n",
    "        feature = preprocess_and_extract(file_path, feature_type)\n",
    "        if feature is not None:\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "            filenames.append(filename)\n",
    "    \n",
    "    return features, labels, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c657f8",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained Models\n",
    "\n",
    "For efficiency, we'll use lightweight pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_metadata['label_encoded'] = label_encoder.fit_transform(train_metadata['primary_label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Total number of classes: {num_classes}\")\n",
    "\n",
    "# Create directory for model checkpoints\n",
    "MODEL_DIR = Path(\"./model_checkpoints\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the label encoder for inference\n",
    "with open(MODEL_DIR / \"label_encoder.pkl\", 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lightweight model architectures optimized for CPU performance\n",
    "\n",
    "# Import pre-trained model functions\n",
    "from torchvision import models\n",
    "import timm\n",
    "\n",
    "# 1. Pre-trained CNN for spectrograms\n",
    "class LocalPretrainedCNN(nn.Module):\n",
    "    def __init__(self, weights_path, num_classes=200):\n",
    "        super(LocalPretrainedCNN, self).__init__()\n",
    "        \n",
    "        # Initialize model without downloading weights\n",
    "        self.model = models.mobilenet_v2(pretrained=False)\n",
    "        \n",
    "        # Load weights from local file\n",
    "        if os.path.exists(weights_path):\n",
    "            self.model.load_state_dict(torch.load(weights_path))\n",
    "            print(f\"Loaded weights from {weights_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Weights file not found at {weights_path}. Using random initialization.\")\n",
    "        \n",
    "        # Modify first layer to accept single channel input (grayscale spectrograms)\n",
    "        first_conv = self.model.features[0][0]\n",
    "        self.model.features[0][0] = nn.Conv2d(\n",
    "            1, first_conv.out_channels,\n",
    "            kernel_size=first_conv.kernel_size,\n",
    "            stride=first_conv.stride,\n",
    "            padding=first_conv.padding,\n",
    "            bias=first_conv.bias is not None\n",
    "        )\n",
    "        \n",
    "        # Replace classifier with a new one for our number of classes\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:  # [batch, freq, time]\n",
    "            x = x.unsqueeze(1)  # [batch, channel, freq, time]\n",
    "        \n",
    "        # Ensure input has at least 32x32 dimensions (minimum for MobileNetV2)\n",
    "        if x.size(2) < 32 or x.size(3) < 32:\n",
    "            x = F.interpolate(x, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "            \n",
    "        return self.model(x)\n",
    "\n",
    "# 2. Lightweight CRNN with pre-trained CNN backbone\n",
    "class LocalPretrainedCRNN(nn.Module):\n",
    "    def __init__(self, weights_path, num_classes=200):\n",
    "        super(LocalPretrainedCRNN, self).__init__()\n",
    "        \n",
    "        # Initialize ResNet18 without downloading weights\n",
    "        self.backbone = models.resnet18(pretrained=False)\n",
    "        \n",
    "        # Load weights from local file\n",
    "        if os.path.exists(weights_path):\n",
    "            self.backbone.load_state_dict(torch.load(weights_path))\n",
    "            print(f\"Loaded weights from {weights_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Weights file not found at {weights_path}. Using random initialization.\")\n",
    "            \n",
    "        # Modify first layer to accept single channel input\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        feature_dim = self.backbone.fc.in_features\n",
    "        \n",
    "        # Remove fully connected layer\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=128,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(256, num_classes)  # 128*2 (bidirectional)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:  # [batch, freq, time]\n",
    "            x = x.unsqueeze(1)  # [batch, channel, freq, time]\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Ensure minimum size for backbone\n",
    "        if x.size(2) < 32 or x.size(3) < 32:\n",
    "            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Extract features with CNN backbone\n",
    "        features = self.backbone(x)  # [batch, feature_dim]\n",
    "        \n",
    "        # Reshape for RNN - treating feature_dim as sequence length\n",
    "        features = features.unsqueeze(1).repeat(1, 16, 1)  # [batch, seq_len=16, feature_dim]\n",
    "        \n",
    "        # Apply RNN\n",
    "        output, _ = self.gru(features)  # [batch, seq_len, 2*hidden_size]\n",
    "        \n",
    "        # Take the last time step\n",
    "        output = output[:, -1, :]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# 3. MLP for MFCC features - this is kept simple and trained from scratch\n",
    "class LightweightMLP(nn.Module):\n",
    "    def __init__(self, input_features=500, num_classes=200):\n",
    "        super(LightweightMLP, self).__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) > 2:\n",
    "            # Flatten if not already flat\n",
    "            x = self.flatten(x)\n",
    "            \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 4. Pre-trained EfficientNet model for improved accuracy\n",
    "class LocalPretrainedEfficientNet(nn.Module):\n",
    "    def __init__(self, weights_path, num_classes=200):\n",
    "        super(LocalPretrainedEfficientNet, self).__init__()\n",
    "        \n",
    "        # Initialize EfficientNet-B0 without downloading weights\n",
    "        self.model = models.efficientnet_b0(pretrained=False)\n",
    "        \n",
    "        # Load weights from local file\n",
    "        if os.path.exists(weights_path):\n",
    "            self.model.load_state_dict(torch.load(weights_path))\n",
    "            print(f\"Loaded weights from {weights_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Weights file not found at {weights_path}. Using random initialization.\")\n",
    "        \n",
    "        # Modify first layer to accept single channel input (grayscale spectrograms)\n",
    "        # EfficientNet has a different structure than MobileNetV2 or ResNet\n",
    "        self.model.features[0][0] = nn.Conv2d(\n",
    "            1, 32,  # EfficientNet-B0 first layer has 32 output channels\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Replace classifier with a new one for our number of classes\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:  # [batch, freq, time]\n",
    "            x = x.unsqueeze(1)  # [batch, channel, freq, time]\n",
    "        \n",
    "        # EfficientNet expects minimum input size of 32x32\n",
    "        if x.size(2) < 32 or x.size(3) < 32:\n",
    "            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd641bd",
   "metadata": {},
   "source": [
    "## 5. CPU-Efficient Training Strategy\n",
    "\n",
    "We'll use a time-efficient approach that combines cached features and pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.FloatTensor(self.features[idx])\n",
    "        label = self.labels[idx]\n",
    "        return feature, label\n",
    "\n",
    "def train_model_efficiently(model, features, labels, val_split=0.2, batch_size=32, epochs=5, model_name=\"model\"):\n",
    "    \"\"\"Train a model with early stopping and minimal overhead\"\"\"\n",
    "    # Encode labels\n",
    "    encoded_labels = label_encoder.transform(labels)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(len(features) * (1 - val_split))\n",
    "    train_features, val_features = features[:split_idx], features[split_idx:]\n",
    "    train_labels, val_labels = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BirdDataset(train_features, train_labels)\n",
    "    val_dataset = BirdDataset(val_features, val_labels)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_loss = float('inf')\n",
    "    patience = 2\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for features, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), MODEL_DIR / f\"{model_name}_best.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(MODEL_DIR / f\"{model_name}_best.pt\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf69239",
   "metadata": {},
   "source": [
    "## 6. Time-Efficient Ensemble Approach\n",
    "\n",
    "We'll use a streamlined ensemble method to maximize accuracy without exceeding the runtime limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad14143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_ensemble(models, weights=None):\n",
    "    \"\"\"Create a simple wrapper for ensemble prediction\"\"\"\n",
    "    if weights is None:\n",
    "        weights = {name: 1/len(models) for name in models}\n",
    "        \n",
    "    def predict(x):\n",
    "        all_probs = []\n",
    "        for name, model in models.items():\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                all_probs.append(weights[name] * probs)\n",
    "                \n",
    "        # Sum weighted probabilities\n",
    "        ensemble_probs = sum(all_probs)\n",
    "        return ensemble_probs\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85aa63d",
   "metadata": {},
   "source": [
    "## 7. Main Training and Prediction Pipeline\n",
    "\n",
    "The main execution pipeline designed to complete within 90 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(max_training_samples=1000, max_test_files=None):\n",
    "    \"\"\"Execute the full pipeline within time constraints\"\"\"\n",
    "    print(\"Starting BirdCLEF 2025 prediction pipeline...\")\n",
    "    \n",
    "    # Check if we're running in Kaggle environment and set paths accordingly\n",
    "    if os.path.exists(\"/kaggle/input\"):\n",
    "        mobilenet_weights_path = \"/kaggle/input/birdclef-pretrained-weights/mobilenet_v2_weights.pth\"\n",
    "        resnet_weights_path = \"/kaggle/input/birdclef-pretrained-weights/resnet18_weights.pth\"\n",
    "        efficientnet_weights_path = \"/kaggle/input/birdclef-pretrained-weights/efficientnet_b0_weights.pth\"\n",
    "        print(f\"Using pre-trained weights from Kaggle dataset\")\n",
    "    else:\n",
    "        # Local paths for testing\n",
    "        mobilenet_weights_path = \"./pretrained_weights/mobilenet_v2_weights.pth\"\n",
    "        resnet_weights_path = \"./pretrained_weights/resnet18_weights.pth\" \n",
    "        efficientnet_weights_path = \"./pretrained_weights/efficientnet_b0_weights.pth\"\n",
    "        print(f\"Using pre-trained weights from local paths\")\n",
    "        \n",
    "    print(f\"MobileNet weights path: {mobilenet_weights_path}\")\n",
    "    print(f\"ResNet weights path: {resnet_weights_path}\")\n",
    "    print(f\"EfficientNet weights path: {efficientnet_weights_path}\")\n",
    "    \n",
    "    # 1. Prepare training data\n",
    "    print(\"\\n[1/5] Preparing training data...\")\n",
    "    train_data_subset = [(row['filename'], row['primary_label']) \n",
    "                         for _, row in train_metadata.iterrows()]\n",
    "    \n",
    "    # Balance the dataset - take equal samples per class if possible\n",
    "    if len(train_data_subset) > max_training_samples:\n",
    "        # Group by class\n",
    "        class_files = {}\n",
    "        for filename, label in train_data_subset:\n",
    "            if label not in class_files:\n",
    "                class_files[label] = []\n",
    "            class_files[label].append((filename, label))\n",
    "            \n",
    "        # Calculate samples per class\n",
    "        samples_per_class = max(1, max_training_samples // len(class_files))\n",
    "        \n",
    "        # Select balanced subset\n",
    "        balanced_subset = []\n",
    "        for label, files in class_files.items():\n",
    "            balanced_subset.extend(files[:samples_per_class])\n",
    "            \n",
    "        # Further trim if still too many\n",
    "        if len(balanced_subset) > max_training_samples:\n",
    "            balanced_subset = balanced_subset[:max_training_samples]\n",
    "            \n",
    "        train_data_subset = balanced_subset\n",
    "    \n",
    "    print(f\"Processing {len(train_data_subset)} training samples...\")\n",
    "    \n",
    "    # 2. Extract features\n",
    "    print(\"\\n[2/5] Extracting features...\")\n",
    "    mel_features, mel_labels, _ = batch_process_files(train_data_subset, feature_type='mel')\n",
    "    mfcc_features, mfcc_labels, _ = batch_process_files(train_data_subset, feature_type='mfcc')\n",
    "    \n",
    "    # 3. Train models\n",
    "    print(\"\\n[3/5] Training models (with local pre-trained weights)...\")\n",
    "    \n",
    "    # Initialize models with local pre-trained weights\n",
    "    cnn_model = LocalPretrainedCNN(weights_path=mobilenet_weights_path, num_classes=num_classes)\n",
    "    crnn_model = LocalPretrainedCRNN(weights_path=resnet_weights_path, num_classes=num_classes)\n",
    "    efficientnet_model = LocalPretrainedEfficientNet(weights_path=efficientnet_weights_path, num_classes=num_classes)\n",
    "    \n",
    "    # Calculate input size for MLP based on actual MFCC feature dimensions\n",
    "    if mfcc_features and len(mfcc_features) > 0:\n",
    "        mfcc_flattened_size = np.prod(mfcc_features[0].shape)\n",
    "        mlp_model = LightweightMLP(input_features=mfcc_flattened_size, num_classes=num_classes)\n",
    "    else:\n",
    "        print(\"No MFCC features available, using default input size\")\n",
    "        mlp_model = LightweightMLP(num_classes=num_classes)\n",
    "    \n",
    "    # For pre-trained models, we can use fewer epochs for fine-tuning\n",
    "    print(\"Fine-tuning pre-trained CNN on mel spectrograms...\")\n",
    "    cnn_model = train_model_efficiently(\n",
    "        cnn_model, mel_features, mel_labels, \n",
    "        epochs=2, batch_size=32, model_name=\"pretrained_cnn\"\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning pre-trained CRNN on mel spectrograms...\")\n",
    "    crnn_model = train_model_efficiently(\n",
    "        crnn_model, mel_features, mel_labels, \n",
    "        epochs=2, batch_size=32, model_name=\"pretrained_crnn\"\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning pre-trained EfficientNet on mel spectrograms...\")\n",
    "    efficientnet_model = train_model_efficiently(\n",
    "        efficientnet_model, mel_features, mel_labels, \n",
    "        epochs=2, batch_size=32, model_name=\"pretrained_efficientnet\"\n",
    "    )\n",
    "    \n",
    "    print(\"Training MLP on MFCCs...\")\n",
    "    mlp_model = train_model_efficiently(\n",
    "        mlp_model, mfcc_features, mfcc_labels, \n",
    "        epochs=3, batch_size=32, model_name=\"mlp\"\n",
    "    )\n",
    "    \n",
    "    # 4. Create ensemble\n",
    "    print(\"\\n[4/5] Creating ensemble model...\")\n",
    "    ensemble_models = {\n",
    "        'cnn': cnn_model,\n",
    "        'crnn': crnn_model,\n",
    "        'efficientnet': efficientnet_model,\n",
    "        'mlp': mlp_model\n",
    "    }\n",
    "    \n",
    "    # Weighting for pre-trained models (giving more weight to pre-trained ones)\n",
    "    ensemble_weights = {\n",
    "        'cnn': 0.30,         # Pre-trained CNN (MobileNetV2)\n",
    "        'crnn': 0.25,        # Pre-trained CRNN with temporal patterns (ResNet18)\n",
    "        'efficientnet': 0.35,# Pre-trained EfficientNet (better performance)\n",
    "        'mlp': 0.10          # MLP is simpler but adds diversity\n",
    "    }\n",
    "    \n",
    "    ensemble_predict = efficient_ensemble(ensemble_models, ensemble_weights)\n",
    "    \n",
    "    # 5. Generate predictions on test data\n",
    "    print(\"\\n[5/5] Generating predictions on test data...\")\n",
    "    test_files = []\n",
    "    \n",
    "    # Find test files\n",
    "    if os.path.exists(TEST_SOUNDSCAPES_DIR):\n",
    "        for file in os.listdir(TEST_SOUNDSCAPES_DIR):\n",
    "            if file.endswith('.ogg') or file.endswith('.wav'):\n",
    "                test_files.append(os.path.join(TEST_SOUNDSCAPES_DIR, file))\n",
    "                \n",
    "        if max_test_files:\n",
    "            test_files = test_files[:max_test_files]\n",
    "            \n",
    "        print(f\"Found {len(test_files)} test files\")\n",
    "    else:\n",
    "        print(f\"Test directory not found: {TEST_SOUNDSCAPES_DIR}\")\n",
    "        print(\"Creating mock test data for demonstration\")\n",
    "        # Create mock test data\n",
    "        test_files = [f\"mock_test_{i}.ogg\" for i in range(10)]\n",
    "    \n",
    "    # Process test files\n",
    "    print(\"Processing test files...\")\n",
    "    all_predictions = []\n",
    "    test_file_ids = []\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        # Get file ID (basename without extension)\n",
    "        file_id = os.path.splitext(os.path.basename(test_file))[0]\n",
    "        test_file_ids.append(file_id)\n",
    "        \n",
    "        # Process audio (in a real scenario)\n",
    "        if os.path.exists(test_file):\n",
    "            # Extract features\n",
    "            mel_features = preprocess_and_extract(test_file, feature_type='mel')\n",
    "            \n",
    "            # Convert to tensor and add batch dimension\n",
    "            mel_tensor = torch.FloatTensor(mel_features).unsqueeze(0)\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            predictions = ensemble_predict(mel_tensor)\n",
    "            all_predictions.append(predictions.squeeze().numpy())\n",
    "        else:\n",
    "            # For demonstration, create random predictions\n",
    "            print(f\"Test file not found, using mock predictions: {test_file}\")\n",
    "            mock_preds = np.random.random(num_classes)\n",
    "            mock_preds = mock_preds / mock_preds.sum()  # Normalize to sum to 1\n",
    "            all_predictions.append(mock_preds)\n",
    "    \n",
    "    # Create submission file\n",
    "    print(\"Creating submission file...\")\n",
    "    submission_entries = []\n",
    "    \n",
    "    # Format according to competition requirements\n",
    "    for file_idx, file_id in enumerate(test_file_ids):\n",
    "        file_preds = all_predictions[file_idx]\n",
    "        \n",
    "        for class_idx, prob in enumerate(file_preds):\n",
    "            species_name = label_encoder.classes_[class_idx]\n",
    "            row_id = f\"{file_id}_{species_name}\"\n",
    "            \n",
    "            submission_entries.append({\n",
    "                \"row_id\": row_id,\n",
    "                \"target\": float(prob)\n",
    "            })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_entries)\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_path = \"submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\\nSubmission file created at {submission_path} with {len(submission_entries)} entries\")\n",
    "    print(\"Sample of submission file:\")\n",
    "    print(submission_df.head())\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full pipeline\n",
    "# Adjust max_training_samples based on available time\n",
    "submission = main_pipeline(max_training_samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f67d0a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implements a CPU-optimized ensemble approach for the BirdCLEF+ 2025 competition, carefully designed to run within the 90-minute runtime limit. The key optimizations include:\n",
    "\n",
    "1. Streamlined preprocessing and feature extraction with caching\n",
    "2. Using pre-trained models (MobileNetV2 and EfficientNet/ResNet) for better starting performance\n",
    "3. Balanced dataset sampling to improve accuracy while reducing computation time\n",
    "4. Efficient ensemble prediction with minimal overhead\n",
    "5. Early stopping to avoid unnecessary training iterations\n",
    "\n",
    "By leveraging transfer learning from pre-trained models while maintaining efficient processing, this solution provides an excellent balance of accuracy and speed for the competition constraints."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
