{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fdb09b",
   "metadata": {},
   "source": [
    "# BirdCLEF+ 2025: CNN on Mel-Spectrograms\n",
    "\n",
    "This notebook implements the CNN on Mel-Spectrograms approach for the BirdCLEF+ 2025 competition. It uses pretrained EfficientNet or ResNet models fine-tuned on mel-spectrograms of animal sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f469a4",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install and import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing packages\n",
    "!pip install -q librosa torchlibrosa timm torchaudio\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm  # For efficient model implementations\n",
    "from torch.cuda.amp import autocast, GradScaler  # For mixed precision training\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# Check if GPU is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba48a1",
   "metadata": {},
   "source": [
    "## Connect to Google Drive\n",
    "\n",
    "Connect to Google Drive to access the dataset and save trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = '/content/drive/MyDrive/birdclef-2025-data'\n",
    "MODEL_SAVE_DIR = '/content/drive/MyDrive/birdclef-2025-models'\n",
    "\n",
    "# Create model save directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"Data path exists: {os.path.exists(DATA_PATH)}\")\n",
    "print(f\"Model save directory: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ad13d",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore the dataset structure and understand the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbe8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata files\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "taxonomy_df = pd.read_csv(os.path.join(DATA_PATH, 'taxonomy.csv'))\n",
    "\n",
    "# Display information\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Unique species: {train_df['primary_label'].nunique()}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a87be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "species_counts = train_df['primary_label'].value_counts()\n",
    "print(f\"Max samples per class: {species_counts.max()}, Min samples per class: {species_counts.min()}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(species_counts)), species_counts.values)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class Index (sorted)')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d01ca",
   "metadata": {},
   "source": [
    "## Audio File Exploration\n",
    "\n",
    "Let's load and visualize some audio files to better understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(file_path, sr=32000):\n",
    "    \"\"\"Load an audio file and convert it to the desired sample rate.\"\"\"\n",
    "    try:\n",
    "        audio, orig_sr = sf.read(file_path)\n",
    "        if orig_sr != sr:\n",
    "            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=sr)\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def plot_audio_and_spectrogram(audio, sr, title=None):\n",
    "    \"\"\"Plot audio waveform and mel-spectrogram.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot waveform\n",
    "    librosa.display.waveshow(audio, sr=sr, ax=axes[0])\n",
    "    axes[0].set_title(f'Waveform ({title})' if title else 'Waveform')\n",
    "    \n",
    "    # Compute and plot mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=sr/2)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    img = librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', fmax=sr/2, ax=axes[1])\n",
    "    axes[1].set_title('Mel-Spectrogram')\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sample a few audio files from different species categories\n",
    "sample_rows = train_df.groupby('primary_label').sample(1).iloc[:3]\n",
    "\n",
    "for idx, row in sample_rows.iterrows():\n",
    "    filename = row['filename']\n",
    "    file_path = os.path.join(DATA_PATH, 'train_audio', filename)\n",
    "    audio, sr = load_audio_file(file_path)\n",
    "    if audio is not None:\n",
    "        print(f\"File: {filename}, Primary Label: {row['primary_label']}, Common Name: {row.get('common_name', 'N/A')}\")\n",
    "        plot_audio_and_spectrogram(audio, sr, title=row.get('common_name', row['primary_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f79152",
   "metadata": {},
   "source": [
    "## Feature Extraction: Mel-Spectrograms\n",
    "\n",
    "Define functions to extract mel-spectrograms from audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melspec(audio, sr=32000, n_mels=128, fmin=20, fmax=16000, window_size=1024, hop_length=512):\n",
    "    \"\"\"Compute a mel-spectrogram from an audio signal.\"\"\"\n",
    "    # Apply a small offset to avoid log(0)\n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        n_fft=window_size,\n",
    "        hop_length=hop_length,\n",
    "        power=2.0\n",
    "    )\n",
    "    \n",
    "    # Convert to dB scale\n",
    "    melspec_db = librosa.power_to_db(melspec, ref=np.max)\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    melspec_normalized = (melspec_db - melspec_db.min()) / (melspec_db.max() - melspec_db.min() + 1e-8)\n",
    "    \n",
    "    return melspec_normalized\n",
    "\n",
    "def audio_to_image(audio, sr=32000, fixed_length=5, **kwargs):\n",
    "    \"\"\"Convert audio to a fixed-length mel-spectrogram 'image'.\"\"\"\n",
    "    # Trim or pad audio to fixed length in seconds\n",
    "    target_len = int(fixed_length * sr)\n",
    "    \n",
    "    if len(audio) > target_len:\n",
    "        # Random crop if audio is longer than target\n",
    "        start = np.random.randint(0, len(audio) - target_len)\n",
    "        audio = audio[start:start + target_len]\n",
    "    else:\n",
    "        # Pad with zeros if audio is shorter than target\n",
    "        padding = target_len - len(audio)\n",
    "        offset = padding // 2\n",
    "        audio = np.pad(audio, (offset, padding - offset), 'constant')\n",
    "    \n",
    "    # Compute mel-spectrogram\n",
    "    melspec = compute_melspec(audio, sr=sr, **kwargs)\n",
    "    \n",
    "    # Add channel dimension for neural network input (1 channel)\n",
    "    return melspec[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a660eb6",
   "metadata": {},
   "source": [
    "## Data Augmentation Functions\n",
    "\n",
    "Define audio augmentation techniques that will be applied during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(audio, shift_factor=0.2):\n",
    "    \"\"\"Apply random time shift to audio.\"\"\"\n",
    "    shift = int(len(audio) * shift_factor)\n",
    "    direction = np.random.randint(0, 2)\n",
    "    if direction == 1:\n",
    "        shift = -shift\n",
    "    aug_audio = np.roll(audio, shift)\n",
    "    # Set the rolled part to zero\n",
    "    if shift > 0:\n",
    "        aug_audio[:shift] = 0\n",
    "    else:\n",
    "        aug_audio[shift:] = 0\n",
    "    return aug_audio\n",
    "\n",
    "def add_gaussian_noise(audio, noise_factor=0.01):\n",
    "    \"\"\"Add random Gaussian noise to audio.\"\"\"\n",
    "    noise = np.random.normal(0, audio.std() * noise_factor, audio.shape)\n",
    "    return audio + noise\n",
    "\n",
    "def change_pitch(audio, sr, pitch_factor=4):\n",
    "    \"\"\"Change pitch of audio without changing tempo.\"\"\"\n",
    "    pitch_shift = np.random.randint(-pitch_factor, pitch_factor)\n",
    "    return librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_shift)\n",
    "\n",
    "def change_speed(audio, speed_factor=0.2):\n",
    "    \"\"\"Change speed of audio.\"\"\"\n",
    "    speed_change = np.random.uniform(1 - speed_factor, 1 + speed_factor)\n",
    "    indices = np.round(np.arange(0, len(audio), speed_change)).astype(int)\n",
    "    indices = indices[indices < len(audio)]\n",
    "    return audio[indices]\n",
    "\n",
    "def apply_augmentation(audio, sr):\n",
    "    \"\"\"Apply a random combination of augmentations to an audio sample.\"\"\"\n",
    "    augmentations = [\n",
    "        lambda audio: time_shift(audio, shift_factor=0.2),\n",
    "        lambda audio: add_gaussian_noise(audio, noise_factor=0.01),\n",
    "        lambda audio: change_pitch(audio, sr, pitch_factor=2),\n",
    "        lambda audio: audio  # Identity function (no augmentation)\n",
    "    ]\n",
    "    \n",
    "    # Randomly select 1-2 augmentations\n",
    "    n_augmentations = np.random.randint(0, 3)  # 0-2 augmentations\n",
    "    augmentation_indices = np.random.choice(len(augmentations), size=n_augmentations, replace=False)\n",
    "    \n",
    "    aug_audio = audio\n",
    "    for idx in augmentation_indices:\n",
    "        aug_audio = augmentations[idx](aug_audio)\n",
    "    \n",
    "    return aug_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8333a6",
   "metadata": {},
   "source": [
    "## Create Dataset and DataLoader\n",
    "\n",
    "Define a custom PyTorch dataset and dataloaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMelDataset(Dataset):\n",
    "    def __init__(self, data_df, data_path, target_sr=32000, duration=5, augment=False, \n",
    "                 cache_waveform=False, cache_melspec=False):\n",
    "        self.data_df = data_df\n",
    "        self.data_path = data_path\n",
    "        self.target_sr = target_sr\n",
    "        self.duration = duration\n",
    "        self.augment = augment\n",
    "        self.cache_waveform = cache_waveform\n",
    "        self.cache_melspec = cache_melspec\n",
    "        self.cached_waveforms = {}\n",
    "        self.cached_melspecs = {}\n",
    "        \n",
    "        # Create label encodings and mapping\n",
    "        self.unique_labels = sorted(data_df['primary_label'].unique())\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.unique_labels)}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "        \n",
    "        # Store file paths for efficient loading\n",
    "        self.file_paths = [os.path.join(data_path, 'train_audio', filename) for filename in data_df['filename']]\n",
    "        self.labels = [self.label_to_idx[label] for label in data_df['primary_label']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Check if we have this waveform cached\n",
    "        if self.cache_waveform and file_path in self.cached_waveforms:\n",
    "            audio = self.cached_waveforms[file_path]\n",
    "            sr = self.target_sr\n",
    "        else:\n",
    "            # Load audio file\n",
    "            audio, sr = load_audio_file(file_path, self.target_sr)\n",
    "            if audio is None:\n",
    "                # If loading failed, create a zero array as fallback\n",
    "                audio = np.zeros(self.target_sr * self.duration)\n",
    "                sr = self.target_sr\n",
    "            \n",
    "            # Cache waveform if enabled\n",
    "            if self.cache_waveform:\n",
    "                self.cached_waveforms[file_path] = audio\n",
    "        \n",
    "        # Generate a unique augmentation key based on audio file and possible random seed\n",
    "        aug_key = None\n",
    "        if self.augment:\n",
    "            # Apply audio augmentation\n",
    "            audio = apply_augmentation(audio, sr)\n",
    "        elif self.cache_melspec:\n",
    "            # For non-augmented data, we can use the file path as the cache key\n",
    "            aug_key = file_path\n",
    "            \n",
    "        # Check if we have the melspec cached (only for non-augmented data)\n",
    "        if self.cache_melspec and aug_key is not None and aug_key in self.cached_melspecs:\n",
    "            melspec = self.cached_melspecs[aug_key]\n",
    "        else:\n",
    "            # Convert to mel-spectrogram\n",
    "            melspec = audio_to_image(audio, sr=sr, fixed_length=self.duration)\n",
    "            \n",
    "            # Cache melspec if enabled and this is non-augmented data\n",
    "            if self.cache_melspec and aug_key is not None:\n",
    "                self.cached_melspecs[aug_key] = melspec\n",
    "        \n",
    "        # Convert to tensor\n",
    "        melspec_tensor = torch.tensor(melspec, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return melspec_tensor, label_tensor\n",
    "\n",
    "\n",
    "def prepare_dataloaders(train_df, data_path, batch_size=32, val_size=0.2, seed=42, \n",
    "                        num_workers=4, cache_waveform=False, cache_val_melspec=True):\n",
    "    \"\"\"Prepare train and validation dataloaders with optimized settings.\"\"\"\n",
    "    # Split data into train and validation sets\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_df, \n",
    "        test_size=val_size, \n",
    "        random_state=seed,\n",
    "        stratify=train_df['primary_label']  # Ensure class balance\n",
    "    )\n",
    "    \n",
    "    # Reset indices\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    val_data = val_data.reset_index(drop=True)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AudioMelDataset(train_data, data_path, augment=True, \n",
    "                                   cache_waveform=cache_waveform, cache_melspec=False)\n",
    "    \n",
    "    # For validation data, we can cache melspectrograms since no augmentation is used\n",
    "    val_dataset = AudioMelDataset(val_data, data_path, augment=False,\n",
    "                                 cache_waveform=cache_waveform, cache_melspec=cache_val_melspec)\n",
    "    \n",
    "    # Create dataloaders with optimal settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        persistent_workers=(num_workers > 0),  # Keep workers alive between batches\n",
    "        prefetch_factor=2 if num_workers > 0 else None,  # Prefetch next batches\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size * 2,  # Can use larger batches for validation\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(num_workers > 0),  # Keep workers alive between batches\n",
    "        prefetch_factor=2 if num_workers > 0 else None,  # Prefetch next batches\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, train_dataset.label_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2eeda",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Define the CNN model using a pre-trained backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMelSpectrogramModel(nn.Module):\n",
    "    def __init__(self, num_classes, backbone=\"efficientnet_b0\", pretrained=True):\n",
    "        super().__init__()\n",
    "        # Load pretrained backbone with proper handling for feature dimension\n",
    "        if \"efficientnet\" in backbone:\n",
    "            # Create the model first to get the feature dimension\n",
    "            temp_model = timm.create_model(\n",
    "                backbone,\n",
    "                pretrained=pretrained,\n",
    "                in_chans=1,  # Grayscale input for mel-spectrogram\n",
    "                num_classes=0  # No classification head\n",
    "            )\n",
    "            # Extract the feature dimension before replacing classifier\n",
    "            feature_dim = temp_model.classifier.in_features\n",
    "            \n",
    "            # Now create the actual model we'll use\n",
    "            self.backbone = timm.create_model(\n",
    "                backbone,\n",
    "                pretrained=pretrained,\n",
    "                in_chans=1,  # Grayscale input for mel-spectrogram\n",
    "                num_classes=0  # No classification head\n",
    "            )\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        elif \"resnet\" in backbone:\n",
    "            # Create the model first to get the feature dimension\n",
    "            temp_model = timm.create_model(\n",
    "                backbone,\n",
    "                pretrained=pretrained,\n",
    "                in_chans=1,  # Grayscale input for mel-spectrogram\n",
    "                num_classes=0  # No classification head\n",
    "            )\n",
    "            # Extract the feature dimension before replacing fc layer\n",
    "            feature_dim = temp_model.fc.in_features\n",
    "            \n",
    "            # Now create the actual model we'll use\n",
    "            self.backbone = timm.create_model(\n",
    "                backbone,\n",
    "                pretrained=pretrained,\n",
    "                in_chans=1,  # Grayscale input for mel-spectrogram\n",
    "                num_classes=0  # No classification head\n",
    "            )\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backbone {backbone} not supported\")\n",
    "        \n",
    "        # New classifier head\n",
    "        self.feature_dim = feature_dim  # Store this for reference\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(feature_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"Model initialized with {backbone} backbone, feature dimension: {feature_dim}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get features from backbone\n",
    "        features = self.backbone(x)\n",
    "        # Apply classifier head\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d97a17",
   "metadata": {},
   "source": [
    "## Training Functions\n",
    "\n",
    "Define training and validation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device, scaler=None, accumulation_steps=1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Reset gradients at the beginning\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for i, (melspec, targets) in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        melspec = melspec.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                outputs = model(melspec)\n",
    "                loss = criterion(outputs, targets) / accumulation_steps  # Normalize loss\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient accumulation - only update weights after accumulation_steps\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            # Standard precision training\n",
    "            outputs = model(melspec)\n",
    "            loss = criterion(outputs, targets) / accumulation_steps  # Normalize loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient accumulation - only update weights after accumulation_steps\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        # Update statistics (use the full loss value for logging)\n",
    "        running_loss += loss.item() * accumulation_steps * melspec.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\"loss\": loss.item() * accumulation_steps, \"acc\": 100. * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "        for melspec, targets in pbar:\n",
    "            # Move data to device\n",
    "            melspec = melspec.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(melspec)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Store outputs and targets for AUC calculation\n",
    "            all_outputs.append(outputs.softmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(F.one_hot(targets, num_classes=outputs.size(1)).cpu().numpy())\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * melspec.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"loss\": loss.item(), \"acc\": 100. * correct / total})\n",
    "    \n",
    "    # Concatenate all outputs and targets\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    # Calculate ROC-AUC (similar to competition metric)\n",
    "    # Only include classes that have positive examples\n",
    "    positive_classes = np.sum(all_targets, axis=0) > 0\n",
    "    if positive_classes.sum() > 0:\n",
    "        roc_auc = roc_auc_score(all_targets[:, positive_classes], all_outputs[:, positive_classes], average='macro')\n",
    "    else:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    return epoch_loss, epoch_acc, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79374563",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Define the main training function with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, \n",
    "          num_epochs=20, patience=5, model_path=\"best_model.pt\"):\n",
    "    \"\"\"Train model with early stopping.\"\"\"\n",
    "    best_val_auc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_auc\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_auc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Update history\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_auc\"].append(val_auc)\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"New best validation AUC: {val_auc:.4f}, saving model...\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epochs. Best Val AUC: {best_val_auc:.4f}\")\n",
    "            \n",
    "            # Check early stopping\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ce611",
   "metadata": {},
   "source": [
    "## Plotting Functions\n",
    "\n",
    "Functions to visualize training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250aa35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history[\"train_loss\"], label=\"Training Loss\")\n",
    "    axes[0].plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Loss Curves\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(history[\"train_acc\"], label=\"Training Accuracy\")\n",
    "    axes[1].plot(history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[1].set_title(\"Accuracy Curves\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Plot AUC\n",
    "    axes[2].plot(history[\"val_auc\"], label=\"Validation AUC\")\n",
    "    axes[2].set_xlabel(\"Epoch\")\n",
    "    axes[2].set_ylabel(\"AUC\")\n",
    "    axes[2].set_title(\"Validation AUC\")\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2ba23",
   "metadata": {},
   "source": [
    "## Main Training\n",
    "\n",
    "Run the training pipeline and save the model for later use in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(save_for_kaggle=True):\n",
    "    # Configuration\n",
    "    BATCH_SIZE = 32  # Smaller base batch size for gradient accumulation\n",
    "    ACCUMULATION_STEPS = 8  # Effective batch size = BATCH_SIZE * ACCUMULATION_STEPS = 256\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "    BACKBONE = \"efficientnet_b0\"  # Or \"resnet34\", \"resnet50\", etc.\n",
    "    NUM_WORKERS = 8  # Increased from 4 to 8 for more efficient data loading\n",
    "    USE_AMP = True  # Enable mixed precision training\n",
    "    CACHE_VAL_MELSPEC = True  # Cache validation mel spectrograms\n",
    "    \n",
    "    # File paths for saving models and data\n",
    "    MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"cnn_melspectrogram_model.pt\")\n",
    "    LABEL_MAPPING_PATH = os.path.join(MODEL_SAVE_DIR, \"label_mapping.pt\")\n",
    "    KAGGLE_EXPORT_PATH = os.path.join(MODEL_SAVE_DIR, \"cnn_melspec_kaggle.pth\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "    \n",
    "    # Prepare dataloaders with optimized settings\n",
    "    train_loader, val_loader, label_to_idx = prepare_dataloaders(\n",
    "        train_df, DATA_PATH, batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS, cache_val_melspec=CACHE_VAL_MELSPEC\n",
    "    )\n",
    "    num_classes = len(label_to_idx)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = CNNMelSpectrogramModel(num_classes, backbone=BACKBONE)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler() if USE_AMP and torch.cuda.is_available() else None\n",
    "    if scaler is not None:\n",
    "        print(\"Using mixed precision training\")\n",
    "    \n",
    "    # Function to wrap training with all optimizations\n",
    "    def optimized_train_epoch(model, train_loader, criterion, optimizer, device, scaler, accumulation_steps):\n",
    "        return train_epoch(model, train_loader, criterion, optimizer, device, scaler, accumulation_steps)\n",
    "    \n",
    "    # Modified training loop that passes the scaler and accumulation steps\n",
    "    def optimized_train(model, train_loader, val_loader, criterion, optimizer, scheduler, device,\n",
    "                        num_epochs=20, patience=5, model_path=\"best_model.pt\",\n",
    "                        scaler=None, accumulation_steps=1):\n",
    "        best_val_auc = 0.0\n",
    "        epochs_without_improvement = 0\n",
    "        history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_auc\": []}\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train for one epoch with optimizations\n",
    "            train_loss, train_acc = optimized_train_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, scaler, accumulation_steps\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, val_auc = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            # Update learning rate\n",
    "            if scheduler is not None:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val AUC: {val_auc:.4f}\")\n",
    "            \n",
    "            # Update history\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_acc\"].append(train_acc)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_acc\"].append(val_acc)\n",
    "            history[\"val_auc\"].append(val_auc)\n",
    "            \n",
    "            # Check if this is the best model\n",
    "            if val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                epochs_without_improvement = 0\n",
    "                print(f\"New best validation AUC: {val_auc:.4f}, saving model...\")\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                print(f\"No improvement for {epochs_without_improvement} epochs. Best Val AUC: {best_val_auc:.4f}\")\n",
    "                \n",
    "                # Check early stopping\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    # Train model with optimizations\n",
    "    model, history = optimized_train(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "        DEVICE, num_epochs=NUM_EPOCHS, model_path=MODEL_PATH,\n",
    "        scaler=scaler, accumulation_steps=ACCUMULATION_STEPS\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Save mapping\n",
    "    idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "    label_mapping = {\"idx_to_label\": idx_to_label}\n",
    "    torch.save(label_mapping, LABEL_MAPPING_PATH)\n",
    "    \n",
    "    # Save a combined model + mapping file for Kaggle\n",
    "    if save_for_kaggle:\n",
    "        kaggle_export = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"backbone\": BACKBONE,\n",
    "            \"label_to_idx\": label_to_idx,\n",
    "            \"idx_to_label\": idx_to_label,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"history\": history\n",
    "        }\n",
    "        torch.save(kaggle_export, KAGGLE_EXPORT_PATH)\n",
    "        print(f\"Model and metadata saved for Kaggle at: {KAGGLE_EXPORT_PATH}\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training pipeline\n",
    "main(save_for_kaggle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10b825",
   "metadata": {},
   "source": [
    "## Inference Functions\n",
    "\n",
    "Define functions for making predictions on new audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac549f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio(model, audio_path, label_to_idx, device=None, sr=32000, duration=5):\n",
    "    \"\"\"Make predictions on a single audio file.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = load_audio_file(audio_path, sr=sr)\n",
    "    if audio is None:\n",
    "        print(f\"Failed to load audio: {audio_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Process audio in overlapping windows\n",
    "    window_duration = duration  # Seconds\n",
    "    window_length = int(window_duration * sr)\n",
    "    step_size = window_length // 2  # 50% overlap\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, max(1, len(audio) - window_length + 1), step_size):\n",
    "            end = min(len(audio), start + window_length)\n",
    "            window = audio[start:end]\n",
    "            \n",
    "            # Convert to mel-spectrogram\n",
    "            melspec = audio_to_image(window, sr=sr, fixed_length=window_duration)\n",
    "            melspec_tensor = torch.tensor(melspec, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            outputs = model(melspec_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "    \n",
    "    # Average predictions from all windows\n",
    "    if all_preds:\n",
    "        avg_preds = np.mean(np.concatenate(all_preds, axis=0), axis=0)\n",
    "        \n",
    "        # Get top predictions\n",
    "        top_indices = np.argsort(-avg_preds)[:5]  # Top 5 predictions\n",
    "        idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "        top_preds = [(idx_to_label[idx], avg_preds[idx]) for idx in top_indices]\n",
    "        \n",
    "        return top_preds\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a9328",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "Make predictions on a few test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbb9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path=\"cnn_melspectrogram_model.pt\", \n",
    "               label_mapping_path=\"label_mapping.pt\", \n",
    "               backbone=\"efficientnet_b0\"):\n",
    "    \"\"\"Test the trained model on a few samples.\"\"\"\n",
    "    # Load label mapping\n",
    "    try:\n",
    "        label_mapping = torch.load(label_mapping_path)\n",
    "        idx_to_label = label_mapping[\"idx_to_label\"]\n",
    "        label_to_idx = {v: k for k, v in idx_to_label.items()}\n",
    "    except FileNotFoundError:\n",
    "        print(\"Label mapping not found. Please train the model first.\")\n",
    "        return\n",
    "    \n",
    "    # Create model\n",
    "    model = CNNMelSpectrogramModel(len(label_to_idx), backbone=backbone)\n",
    "    \n",
    "    # Load model weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Sample a few test files\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "    test_samples = train_df.sample(5)\n",
    "    \n",
    "    for idx, row in test_samples.iterrows():\n",
    "        file_path = os.path.join(DATA_PATH, 'train_audio', row['filename'])\n",
    "        print(f\"\\nTesting: {row['filename']}\")\n",
    "        print(f\"True label: {row['primary_label']}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = predict_audio(model, file_path, label_to_idx, device=DEVICE)\n",
    "        \n",
    "        if predictions:\n",
    "            print(\"Top predictions:\")\n",
    "            for label, prob in predictions:\n",
    "                print(f\"{label}: {prob:.4f}\")\n",
    "        else:\n",
    "            print(\"Failed to make prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# test_model()  # Uncomment to test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15e4ad",
   "metadata": {},
   "source": [
    "## Verify Model for Kaggle\n",
    "\n",
    "Let's verify that the saved model can be loaded and used correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_kaggle_model(model_path=None):\n",
    "    \"\"\"Test loading the model saved for Kaggle.\"\"\"\n",
    "    if model_path is None:\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, \"cnn_melspec_kaggle.pth\")\n",
    "    \n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    \n",
    "    # Load the saved model data\n",
    "    try:\n",
    "        kaggle_data = torch.load(model_path)\n",
    "        print(\"Successfully loaded model data!\")\n",
    "        \n",
    "        # Extract metadata\n",
    "        backbone = kaggle_data.get(\"backbone\", \"efficientnet_b0\")\n",
    "        num_classes = kaggle_data.get(\"num_classes\")\n",
    "        \n",
    "        print(f\"Model information:\")\n",
    "        print(f\"- Backbone: {backbone}\")\n",
    "        print(f\"- Number of classes: {num_classes}\")\n",
    "        print(f\"- Label mapping available: {'Yes' if 'idx_to_label' in kaggle_data else 'No'}\")\n",
    "        \n",
    "        # Create a new model instance\n",
    "        model = CNNMelSpectrogramModel(num_classes, backbone=backbone)\n",
    "        \n",
    "        # Load the state dict\n",
    "        model.load_state_dict(kaggle_data[\"model_state_dict\"])\n",
    "        print(\"Model weights loaded successfully!\")\n",
    "        \n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        print(\"Model ready for inference!\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7164aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the model can be loaded for Kaggle\n",
    "verify_kaggle_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873bb80",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implemented a CNN model on mel-spectrograms for the BirdCLEF+ 2025 competition. The model uses a pre-trained EfficientNet or ResNet backbone fine-tuned on mel-spectrograms of bird and animal sounds. Key features include:\n",
    "\n",
    "1. Comprehensive audio preprocessing for mel-spectrogram generation\n",
    "2. Data augmentation techniques for audio (time shift, noise, pitch shift)\n",
    "3. Custom dataset and dataloader implementation\n",
    "4. Pre-trained CNN backbone with transfer learning\n",
    "5. Training pipeline with early stopping and learning rate scheduling\n",
    "6. ROC-AUC evaluation matching competition metric\n",
    "7. Inference with window-based processing for longer audio files\n",
    "\n",
    "After training, the model is saved in two formats:\n",
    "1. Separate model weights and label mapping files\n",
    "2. A combined file for Kaggle with all necessary data for inference\n",
    "\n",
    "This model represents one component of the ensemble approach described in the project proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea4252",
   "metadata": {},
   "source": [
    "## Loading Precomputed Features\n",
    "\n",
    "Load precomputed mel spectrograms from the shared NPZ file instead of generating them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d691351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the precomputed features file\n",
    "# First, download from Google Drive if needed\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# URL for the shared Google Drive file\n",
    "PRECOMPUTED_FEATURES_URL = \"https://drive.google.com/file/d/1bkkglM6lV1aV-9bSsVpD475YewPwblmL/view?usp=sharing\"\n",
    "PRECOMPUTED_FEATURES_PATH = os.path.join(DATA_PATH, 'bird_features_scratch.npz')\n",
    "\n",
    "def load_precomputed_features(url=PRECOMPUTED_FEATURES_URL, local_path=PRECOMPUTED_FEATURES_PATH):\n",
    "    \"\"\"Download and load precomputed features from Google Drive.\"\"\"\n",
    "    # Check if file already exists\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Downloading precomputed features file to {local_path}...\")\n",
    "        # Use gdown to download the file from Google Drive\n",
    "        gdown.download(url, local_path, quiet=False, fuzzy=True)\n",
    "    else:\n",
    "        print(f\"Precomputed features file already exists at {local_path}\")\n",
    "    \n",
    "    # Load the features\n",
    "    print(\"Loading precomputed features...\")\n",
    "    try:\n",
    "        data = np.load(local_path)\n",
    "        X = data['data']\n",
    "        y = data['labels']\n",
    "        print(f\"Successfully loaded precomputed features!\")\n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "        print(f\"Number of unique classes: {len(np.unique(y))}\")\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading precomputed features: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load the precomputed features\n",
    "X_precomputed, y_precomputed = load_precomputed_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47fe8b",
   "metadata": {},
   "source": [
    "## Training with Precomputed Features\n",
    "\n",
    "Let's create a more efficient training pipeline using the precomputed features instead of generating them on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precomputed_features_training(X_features=None, y_labels=None, test_size=0.2, seed=42):\n",
    "    \"\"\"Set up and train the model using precomputed features instead of generating them on the fly.\"\"\"\n",
    "    # If features aren't provided, load them first\n",
    "    if X_features is None or y_labels is None:\n",
    "        X_features, y_labels = load_precomputed_features()\n",
    "        if X_features is None:\n",
    "            print(\"Failed to load precomputed features. Exiting.\")\n",
    "            return None, None\n",
    "    \n",
    "    print(\"Setting up training with precomputed features...\")\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_features, y_labels, test_size=test_size, random_state=seed, stratify=y_labels)\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # Use larger batch sizes since we're not generating features on the fly\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Configuration\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "    BACKBONE = \"efficientnet_b0\"\n",
    "    MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"cnn_melspectrogram_precomputed.pt\")\n",
    "    KAGGLE_EXPORT_PATH = os.path.join(MODEL_SAVE_DIR, \"cnn_melspec_precomputed_kaggle.pth\")\n",
    "    \n",
    "    # Get number of unique classes\n",
    "    num_classes = len(np.unique(y_labels))\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Create a mapping from index to label\n",
    "    # Since we just have numerical labels from the NPZ file, we'll create a simple mapping\n",
    "    idx_to_label = {int(idx): f\"class_{idx}\" for idx in range(num_classes)}\n",
    "    label_to_idx = {v: k for k, v in idx_to_label.items()}\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = CNNMelSpectrogramModel(num_classes, backbone=BACKBONE)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Loss function, optimizer and scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # Use mixed precision training if available\n",
    "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "    if scaler is not None:\n",
    "        print(\"Using mixed precision training\")\n",
    "    \n",
    "    # Start training\n",
    "    model, history = train(model, train_loader, val_loader, criterion, optimizer, scheduler, DEVICE, \n",
    "                          num_epochs=NUM_EPOCHS, patience=5, model_path=MODEL_PATH)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Save model for Kaggle\n",
    "    kaggle_export = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"backbone\": BACKBONE,\n",
    "        \"label_to_idx\": label_to_idx,\n",
    "        \"idx_to_label\": idx_to_label,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"history\": history\n",
    "    }\n",
    "    torch.save(kaggle_export, KAGGLE_EXPORT_PATH)\n",
    "    print(f\"Model and metadata saved for Kaggle at: {KAGGLE_EXPORT_PATH}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ed167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training with precomputed features\n",
    "model_precomputed, history_precomputed = precomputed_features_training(X_precomputed, y_precomputed)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
