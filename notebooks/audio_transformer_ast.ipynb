{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aad8c80",
   "metadata": {},
   "source": [
    "# BirdCLEF+ 2025: Audio Spectrogram Transformer (AST) Model\n",
    "\n",
    "This notebook implements the Audio Transformer component of our ensemble approach for the BirdCLEF+ 2025 competition. We leverage the pretrained Audio Spectrogram Transformer (AST) model, which has shown excellent performance on audio classification tasks.\n",
    "\n",
    "## Strategy\n",
    "- Use the pretrained AST model as a foundation\n",
    "- Fine-tune the model on the BirdCLEF+ 2025 dataset\n",
    "- Implement cross-validation to ensure robust performance\n",
    "- Add appropriate audio augmentations to handle limited data\n",
    "- Generate predictions that can be ensembled with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c443f6",
   "metadata": {},
   "source": [
    "## Setup Google Colab Environment\n",
    "\n",
    "First, we'll set up the Colab environment by mounting Google Drive and installing required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba119c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers timm torchaudio librosa matplotlib scikit-learn\n",
    "\n",
    "# Set up paths for Google Drive\n",
    "import os\n",
    "DATA_DIR = '/content/drive/MyDrive/birdclef-2025-data'\n",
    "MODEL_SAVE_DIR = '/content/drive/MyDrive/fp-561-models'\n",
    "    \n",
    "# Create model save directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "print(f\"Data directory exists: {os.path.exists(DATA_DIR)}\")\n",
    "print(f\"Model save directory: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21722a1",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import transformers\n",
    "from transformers import ASTModel, ASTForAudioClassification, AutoFeatureExtractor\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7175ba6",
   "metadata": {},
   "source": [
    "## Set Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21191e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "TRAIN_AUDIO_DIR = os.path.join(DATA_DIR, 'train_audio')\n",
    "TRAIN_CSV_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "TAXONOMY_PATH = os.path.join(DATA_DIR, 'taxonomy.csv')\n",
    "SOUNDSCAPE_DIR = os.path.join(DATA_DIR, 'train_soundscapes')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_folds': 5,\n",
    "    'model_name': 'MIT/ast-finetuned-audioset-10-10-0.4593',  # Pretrained AST model\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 8,  # Smaller batch size to fit in memory\n",
    "    'learning_rate': 1e-5,  # Low LR for fine-tuning\n",
    "    'weight_decay': 1e-4,\n",
    "    'max_audio_len': 10,  # seconds\n",
    "    'sample_rate': 32000,  # BirdCLEF dataset sample rate\n",
    "    'early_stopping_patience': 5,\n",
    "    'target_sample_rate': 16000,  # AST model expects 16kHz\n",
    "    'use_mixup': True,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'audio_max_length': 10,\n",
    "    'model_save_dir': MODEL_SAVE_DIR  # Save models to Google Drive\n",
    "}\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the input files and directories exist\n",
    "print(f\"Train CSV exists: {os.path.exists(TRAIN_CSV_PATH)}\")\n",
    "print(f\"Taxonomy CSV exists: {os.path.exists(TAXONOMY_PATH)}\")\n",
    "print(f\"Train audio directory exists: {os.path.exists(TRAIN_AUDIO_DIR)}\")\n",
    "\n",
    "# Check how many audio files are available\n",
    "if os.path.exists(TRAIN_AUDIO_DIR):\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(TRAIN_AUDIO_DIR):\n",
    "        audio_files.extend([os.path.join(root, file) for file in files if file.endswith('.ogg')])\n",
    "    print(f\"Found {len(audio_files)} audio files in {TRAIN_AUDIO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c34b51",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data and taxonomy\n",
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "df_taxonomy = pd.read_csv(TAXONOMY_PATH)\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"Number of unique species: {df_train['primary_label'].nunique()}\")\n",
    "\n",
    "# Look at class distribution\n",
    "class_counts = df_train['primary_label'].value_counts()\n",
    "print(f\"Class distribution - min: {class_counts.min()}, max: {class_counts.max()}, mean: {class_counts.mean():.2f}\")\n",
    "\n",
    "# View a few rows\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f24fd7",
   "metadata": {},
   "source": [
    "## Create Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique species labels (primary + secondary)\n",
    "def get_all_species_labels():\n",
    "    primary_labels = df_train['primary_label'].unique().tolist()\n",
    "    \n",
    "    # Extract secondary labels\n",
    "    secondary_labels = []\n",
    "    for labels in df_train['secondary_labels'].fillna('[]'):\n",
    "        try:\n",
    "            secondary_labels.extend(eval(labels))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Combine all unique labels\n",
    "    all_labels = sorted(set(primary_labels + secondary_labels))\n",
    "    \n",
    "    # Create label encoders\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(all_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    \n",
    "    return all_labels, label_to_idx, idx_to_label\n",
    "\n",
    "all_labels, label_to_idx, idx_to_label = get_all_species_labels()\n",
    "num_classes = len(all_labels)\n",
    "\n",
    "print(f\"Total number of unique species: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc3976",
   "metadata": {},
   "source": [
    "## Audio Processing and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2126aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature extractor for the AST model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "# Audio augmentation functions\n",
    "def random_power(audio, power_min=0.8, power_max=1.2):\n",
    "    # Apply random power to audio (volume augmentation)\n",
    "    power = torch.tensor(random.uniform(power_min, power_max))\n",
    "    return audio ** power\n",
    "\n",
    "def add_white_noise(audio, noise_factor_min=0.001, noise_factor_max=0.01):\n",
    "    # Add random white noise\n",
    "    noise_factor = random.uniform(noise_factor_min, noise_factor_max)\n",
    "    noise = torch.randn_like(audio) * noise_factor\n",
    "    return audio + noise\n",
    "\n",
    "def random_time_shift(audio, shift_factor=0.2):\n",
    "    # Randomly shift audio in time (cyclic)\n",
    "    shift = int(random.uniform(-shift_factor, shift_factor) * len(audio))\n",
    "    return torch.roll(audio, shift)\n",
    "\n",
    "def mixup(audio1, audio2, labels1, labels2, alpha=0.2):\n",
    "    # Mixup augmentation - blend two audio samples\n",
    "    lambda_param = np.random.beta(alpha, alpha)\n",
    "    lambda_param = max(lambda_param, 1-lambda_param)\n",
    "    \n",
    "    mixed_audio = lambda_param * audio1 + (1 - lambda_param) * audio2\n",
    "    mixed_labels = lambda_param * labels1 + (1 - lambda_param) * labels2\n",
    "    \n",
    "    return mixed_audio, mixed_labels\n",
    "\n",
    "# Custom dataset class for audio\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, df, label_to_idx, audio_dir, is_train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.is_train = is_train\n",
    "        self.target_sample_rate = CONFIG['target_sample_rate']\n",
    "        self.max_audio_len = CONFIG['audio_max_length']\n",
    "        self.num_classes = len(label_to_idx)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = os.path.join(self.audio_dir, row['filename'])\n",
    "        \n",
    "        # Load audio\n",
    "        try:\n",
    "            audio, sample_rate = librosa.load(audio_path, sr=self.target_sample_rate, mono=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {e}\")\n",
    "            # Use empty audio as fallback\n",
    "            audio = np.zeros(self.target_sample_rate * self.max_audio_len)\n",
    "        \n",
    "        # Process the audio to fixed length\n",
    "        max_samples = int(self.target_sample_rate * self.max_audio_len)\n",
    "        \n",
    "        if len(audio) > max_samples:\n",
    "            # For training, take a random segment\n",
    "            if self.is_train:\n",
    "                start = np.random.randint(0, len(audio) - max_samples)\n",
    "                audio = audio[start:start + max_samples]\n",
    "            else:\n",
    "                # For validation/test, take the beginning\n",
    "                audio = audio[:max_samples]\n",
    "        else:\n",
    "            # If audio is too short, pad with zeros\n",
    "            padding = max_samples - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode='constant')\n",
    "        \n",
    "        # Convert to tensor\n",
    "        audio_tensor = torch.tensor(audio, dtype=torch.float32)\n",
    "        \n",
    "        # Apply augmentations during training\n",
    "        if self.is_train:\n",
    "            if random.random() > 0.5:\n",
    "                audio_tensor = random_power(audio_tensor)\n",
    "            if random.random() > 0.7:\n",
    "                audio_tensor = add_white_noise(audio_tensor)\n",
    "            if random.random() > 0.5:\n",
    "                audio_tensor = random_time_shift(audio_tensor)\n",
    "        \n",
    "        # Create one-hot encoded target\n",
    "        primary_label = row['primary_label']\n",
    "        primary_idx = self.label_to_idx.get(primary_label, 0)\n",
    "        \n",
    "        target = torch.zeros(self.num_classes)\n",
    "        target[primary_idx] = 1.0\n",
    "        \n",
    "        # Add secondary labels if available\n",
    "        if 'secondary_labels' in row and isinstance(row['secondary_labels'], str) and len(row['secondary_labels']) > 2:\n",
    "            try:\n",
    "                secondary_labels = eval(row['secondary_labels'])\n",
    "                for label in secondary_labels:\n",
    "                    if label in self.label_to_idx:\n",
    "                        target[self.label_to_idx[label]] = 1.0\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Process with AST feature extractor\n",
    "        audio_inputs = feature_extractor(\n",
    "            audio_tensor, \n",
    "            sampling_rate=self.target_sample_rate, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=feature_extractor.max_length if hasattr(feature_extractor, 'max_length') else None,\n",
    "            padding=\"max_length\" if hasattr(feature_extractor, 'max_length') else \"do_not_pad\",\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_values': audio_inputs.input_values.squeeze(),\n",
    "            'attention_mask': audio_inputs.attention_mask.squeeze() if hasattr(audio_inputs, 'attention_mask') else None,\n",
    "            'target': target,\n",
    "            'primary_idx': primary_idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdde7fc",
   "metadata": {},
   "source": [
    "## Data Loaders & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collate function that handles batching\n",
    "def collate_fn(batch):\n",
    "    input_values = torch.stack([item['input_values'] for item in batch])\n",
    "    targets = torch.stack([item['target'] for item in batch])\n",
    "    primary_idx = torch.tensor([item['primary_idx'] for item in batch])\n",
    "    \n",
    "    # Handle attention_mask (may be None for some models)\n",
    "    if batch[0]['attention_mask'] is not None:\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    else:\n",
    "        attention_mask = None\n",
    "    \n",
    "    return {\n",
    "        'input_values': input_values,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target': targets,\n",
    "        'primary_idx': primary_idx\n",
    "    }\n",
    "\n",
    "# Data loader with mixup\n",
    "class MixupDataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True, num_workers=2, mixup_alpha=0.2):\n",
    "        self.dataloader = DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle, \n",
    "            num_workers=num_workers, collate_fn=collate_fn\n",
    "        )\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            if CONFIG['use_mixup']:\n",
    "                # Apply mixup to half of the batches\n",
    "                if np.random.random() > 0.5:\n",
    "                    # Create a shuffled index\n",
    "                    batch_size = len(batch['target'])\n",
    "                    shuffled_idx = torch.randperm(batch_size)\n",
    "                    \n",
    "                    # Get lambda parameter for mixup\n",
    "                    lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "                    lam = max(lam, 1-lam)  # Ensure the stronger signal has a higher weight\n",
    "                    \n",
    "                    # Mix inputs\n",
    "                    mixed_input = lam * batch['input_values'] + (1 - lam) * batch['input_values'][shuffled_idx]\n",
    "                    batch['input_values'] = mixed_input\n",
    "                    \n",
    "                    # Mix targets\n",
    "                    mixed_target = lam * batch['target'] + (1 - lam) * batch['target'][shuffled_idx]\n",
    "                    batch['target'] = mixed_target\n",
    "                    \n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f07c3",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21689b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, freeze_base=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load the pretrained AST model\n",
    "        self.base_model = ASTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Get the hidden size\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        \n",
    "        # Freeze the base model if specified\n",
    "        if freeze_base:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Create new classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        outputs = self.base_model(\n",
    "            input_values=input_values,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get the [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Pass through the classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19368dcd",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ed5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def get_loss_fn():\n",
    "    return nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "def get_optimizer(model):\n",
    "    return torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
    "    return transformers.get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device) if batch['attention_mask'] is not None else None\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "            outputs = model(input_values, attention_mask)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            all_targets.append(targets)\n",
    "            all_preds.append(preds)\n",
    "    \n",
    "    all_targets = np.vstack(all_targets)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    \n",
    "    # Compute ROC-AUC per class and then macro average\n",
    "    aucs = []\n",
    "    for i in range(all_targets.shape[1]):\n",
    "        # Skip if no positive samples\n",
    "        if np.sum(all_targets[:, i]) > 0:\n",
    "            aucs.append(roc_auc_score(all_targets[:, i], all_preds[:, i]))\n",
    "    \n",
    "    mean_auc = np.mean(aucs) if len(aucs) > 0 else 0\n",
    "    return mean_auc, all_preds, all_targets\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, loss_fn, device):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_values = batch['input_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device) if batch['attention_mask'] is not None else None\n",
    "        targets = batch['target'].to(device)\n",
    "        \n",
    "        outputs = model(input_values, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ad99e",
   "metadata": {},
   "source": [
    "## Cross-Validation and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1fc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified folds\n",
    "skf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "df_train['fold'] = -1\n",
    "\n",
    "# Assign folds\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train['primary_label'])):\n",
    "    df_train.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(f\"Fold distribution:\\n{df_train['fold'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32e708",
   "metadata": {},
   "source": [
    "## Train Model with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a56af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(fold=0):\n",
    "    # Create train and validation datasets\n",
    "    train_df = df_train[df_train['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = df_train[df_train['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Fold {fold}: Train size: {len(train_df)}, Valid size: {len(valid_df)}\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = BirdCLEFDataset(train_df, label_to_idx, TRAIN_AUDIO_DIR, is_train=True)\n",
    "    valid_dataset = BirdCLEFDataset(valid_df, label_to_idx, TRAIN_AUDIO_DIR, is_train=False)\n",
    "    \n",
    "    train_loader = MixupDataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        mixup_alpha=CONFIG['mixup_alpha']\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = ASTClassifier(\n",
    "        model_name=CONFIG['model_name'], \n",
    "        num_classes=num_classes,\n",
    "        freeze_base=False  # Start with full fine-tuning\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Create optimizer and loss function\n",
    "    optimizer = get_optimizer(model)\n",
    "    loss_fn = get_loss_fn()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    num_training_steps = len(train_loader) * CONFIG['num_epochs']\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)\n",
    "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    best_auc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    model_path = os.path.join(CONFIG['model_save_dir'], f\"ast_fold_{fold}.pt\")\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, loss_fn, DEVICE)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_auc, val_preds, val_targets = evaluate(model, valid_loader, DEVICE)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val ROC-AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Saved best model with ROC-AUC: {best_auc:.4f} to {model_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs. Best ROC-AUC: {best_auc:.4f} at epoch {best_epoch+1}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Save checkpoint every 5 epochs to prevent loss of progress in case of Colab disconnection\n",
    "        if epoch > 0 and (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = os.path.join(CONFIG['model_save_dir'], f\"ast_fold_{fold}_checkpoint_epoch_{epoch+1}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_auc': best_auc\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model, best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76bb27",
   "metadata": {},
   "source": [
    "## Train Models for Multiple Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f50a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for the first fold (adjust as needed for all folds)\n",
    "fold_to_train = 0  # Change this to train different folds\n",
    "\n",
    "model, best_auc = train_and_validate(fold=fold_to_train)\n",
    "print(f\"Fold {fold_to_train} Best ROC-AUC: {best_auc:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = os.path.join(CONFIG['model_save_dir'], f\"ast_final_fold_{fold_to_train}.pt\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b97f5",
   "metadata": {},
   "source": [
    "## Inference and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_for_inference(audio_path, target_sr=16000, max_length_seconds=10):\n",
    "    # Load and preprocess audio file for inference\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=target_sr, mono=True)\n",
    "        max_len = int(target_sr * max_length_seconds)\n",
    "        \n",
    "        # If audio is longer than max_length_seconds, take multiple segments\n",
    "        if len(audio) > max_len:\n",
    "            # Divide the audio into overlapping segments\n",
    "            segments = []\n",
    "            hop_length = int(max_len * 0.5)  # 50% overlap\n",
    "            \n",
    "            for start in range(0, len(audio) - max_len + 1, hop_length):\n",
    "                segment = audio[start:start + max_len]\n",
    "                segments.append(segment)\n",
    "            \n",
    "            # Add the last segment if it's not already included\n",
    "            if len(audio) > max_len and (len(audio) - max_len) % hop_length != 0:\n",
    "                segments.append(audio[-max_len:])\n",
    "                \n",
    "            return segments, sr\n",
    "        else:\n",
    "            # If audio is shorter than max_length_seconds, pad it\n",
    "            if len(audio) < max_len:\n",
    "                padding = max_len - len(audio)\n",
    "                audio = np.pad(audio, (0, padding), mode='constant')\n",
    "            \n",
    "            return [audio], sr\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio {audio_path}: {e}\")\n",
    "        return [np.zeros(target_sr * max_length_seconds)], target_sr\n",
    "\n",
    "def predict_audio(model, audio_path):\n",
    "    model.eval()\n",
    "    segments, sr = load_audio_for_inference(audio_path)\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for segment in segments:\n",
    "            # Convert to tensor\n",
    "            audio_tensor = torch.tensor(segment, dtype=torch.float32)\n",
    "            \n",
    "            # Process with feature extractor\n",
    "            audio_inputs = feature_extractor(\n",
    "                audio_tensor, \n",
    "                sampling_rate=sr, \n",
    "                return_tensors=\"pt\",\n",
    "                max_length=feature_extractor.max_length if hasattr(feature_extractor, 'max_length') else None,\n",
    "                padding=\"max_length\" if hasattr(feature_extractor, 'max_length') else \"do_not_pad\",\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Get model predictions\n",
    "            input_values = audio_inputs.input_values.to(DEVICE)\n",
    "            attention_mask = audio_inputs.attention_mask.to(DEVICE) if hasattr(audio_inputs, 'attention_mask') else None\n",
    "            \n",
    "            outputs = model(input_values, attention_mask)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "    \n",
    "    # Average the predictions from all segments\n",
    "    avg_preds = np.mean(all_preds, axis=0)[0]\n",
    "    \n",
    "    return avg_preds\n",
    "\n",
    "# Function to create predictions for Kaggle submission\n",
    "def create_predictions(model, test_dir, output_file='submission.csv'):\n",
    "    # Get list of test files\n",
    "    test_files = glob.glob(os.path.join(test_dir, '*.ogg'))\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test files\")\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for audio_file in tqdm(test_files, desc=\"Predicting\"):\n",
    "        file_id = os.path.basename(audio_file).split('.')[0]\n",
    "        preds = predict_audio(model, audio_file)\n",
    "        \n",
    "        # Create a row for each 5-second segment\n",
    "        for end_time in range(5, 65, 5):  # 5, 10, 15, ..., 60\n",
    "            row_id = f\"{file_id}_{end_time}\"\n",
    "            predictions.append([row_id] + list(preds))\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    columns = ['row_id'] + list(idx_to_label.values())\n",
    "    submission_df = pd.DataFrame(predictions, columns=columns)\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission_path = os.path.join(CONFIG['model_save_dir'], output_file)\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Predictions saved to {submission_path}\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f478c",
   "metadata": {},
   "source": [
    "## Analyze Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate our model performance per species\n",
    "def analyze_performance_per_species(fold=0):\n",
    "    # Create validation dataset\n",
    "    valid_df = df_train[df_train['fold'] == fold].reset_index(drop=True)\n",
    "    valid_dataset = BirdCLEFDataset(valid_df, label_to_idx, TRAIN_AUDIO_DIR, is_train=False)\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Load trained model\n",
    "    model = ASTClassifier(CONFIG['model_name'], num_classes).to(DEVICE)\n",
    "    model_path = os.path.join(CONFIG['model_save_dir'], f\"ast_fold_{fold}.pt\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    # Get predictions\n",
    "    _, val_preds, val_targets = evaluate(model, valid_loader, DEVICE)\n",
    "    \n",
    "    # Calculate AUC per species\n",
    "    species_aucs = {}\n",
    "    for i, species_name in idx_to_label.items():\n",
    "        if np.sum(val_targets[:, i]) > 0:  # Only include species with positive samples\n",
    "            auc = roc_auc_score(val_targets[:, i], val_preds[:, i])\n",
    "            species_aucs[species_name] = auc\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    species_perf_df = pd.DataFrame({\n",
    "        'species': list(species_aucs.keys()),\n",
    "        'auc': list(species_aucs.values())\n",
    "    }).sort_values('auc')\n",
    "    \n",
    "    # Show best and worst performing species\n",
    "    print(\"Best performing species:\")\n",
    "    print(species_perf_df.tail(10))\n",
    "    \n",
    "    print(\"\\nWorst performing species:\")\n",
    "    print(species_perf_df.head(10))\n",
    "    \n",
    "    # Plot AUC distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(species_perf_df['auc'], bins=20)\n",
    "    plt.xlabel('ROC AUC')\n",
    "    plt.ylabel('Number of Species')\n",
    "    plt.title('Distribution of ROC AUC scores across species')\n",
    "    plt.savefig(os.path.join(CONFIG['model_save_dir'], f'species_auc_distribution_fold_{fold}.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Save species performance to CSV\n",
    "    species_perf_path = os.path.join(CONFIG['model_save_dir'], f'species_performance_fold_{fold}.csv')\n",
    "    species_perf_df.to_csv(species_perf_path, index=False)\n",
    "    print(f\"Species performance saved to {species_perf_path}\")\n",
    "    \n",
    "    return species_perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, run this to analyze model performance\n",
    "# species_performance = analyze_performance_per_species(fold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f2265",
   "metadata": {},
   "source": [
    "## Prepare Model for Ensemble\n",
    "\n",
    "We'll save some metadata about the model to help with ensembling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_info(fold=0):\n",
    "    \"\"\"Save model metadata for later ensemble use\"\"\"\n",
    "    model_info = {\n",
    "        'model_type': 'AST',\n",
    "        'base_model': CONFIG['model_name'],\n",
    "        'fold': fold,\n",
    "        'num_classes': num_classes,\n",
    "        'label_to_idx': label_to_idx,\n",
    "        'idx_to_label': idx_to_label,\n",
    "        'input_sample_rate': CONFIG['target_sample_rate'],\n",
    "        'date_trained': pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    import json\n",
    "    with open(os.path.join(CONFIG['model_save_dir'], f'ast_model_info_fold_{fold}.json'), 'w') as f:\n",
    "        # Convert non-serializable items to strings\n",
    "        model_info_serializable = {k: str(v) if not isinstance(v, (dict, list, str, int, float, bool, type(None))) else v \n",
    "                                for k, v in model_info.items()}\n",
    "        json.dump(model_info_serializable, f, indent=2)\n",
    "    \n",
    "    print(f\"Model info saved for fold {fold}\")\n",
    "    return model_info\n",
    "\n",
    "# Save model info after training\n",
    "# model_info = save_model_info(fold=fold_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b64dc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implements the Audio Transformer component (using a pretrained AST model) for the BirdCLEF+ 2025 ensemble approach. Key features include:\n",
    "\n",
    "1. Leveraging the pretrained Audio Spectrogram Transformer (AST) for audio classification\n",
    "2. Comprehensive audio augmentation techniques including mixup, time shift, and noise addition\n",
    "3. Multi-label classification capability for detecting multiple species in a single recording\n",
    "4. Proper cross-validation strategy with stratified folds\n",
    "5. Specialized inference handling for long audio files using overlapping windows\n",
    "6. Integration with Google Drive for persistent storage of models and results\n",
    "\n",
    "Next steps:\n",
    "- Train models for all folds\n",
    "- Combine with other models in the ensemble (CNN, CRNN, etc.)\n",
    "- Further explore semi-supervised learning with the unlabeled soundscape data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
