{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692102b9",
   "metadata": {},
   "source": [
    "# BirdCLEF+ 2025: Audio Transformer Model\n",
    "\n",
    "This notebook implements a transformer-based architecture for the BirdCLEF+ 2025 competition. The transformer model is designed to capture long-range context in audio data, particularly useful for detecting complex patterns like overlapping calls from multiple species.\n",
    "\n",
    "As described in the project strategy, this transformer model provides a different inductive bias compared to CNNs (global self-attention vs. local convolution), enriching the ensemble approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q librosa torchlibrosa timm torchaudio einops\n",
    "\n",
    "# Imports\n",
    "import os, random, warnings, numpy as np, pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, librosa.display, soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility and device setup\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed); torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "print(f'Current time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4669cda",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load training metadata and define paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../birdclef-2025-data'  # adjust if needed\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "print(f'Training samples: {len(train_df)}')\n",
    "\n",
    "# Get unique species count\n",
    "species = train_df['primary_label'].unique()\n",
    "print(f'Number of unique species: {len(species)}')\n",
    "\n",
    "# Check class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "train_df['primary_label'].value_counts().head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Species Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Species')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b797c",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Define mel-spectrogram extraction functions. For transformers, we'll use a denser representation with more time frames to capture long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e87231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melspec(audio, sr=32000, n_mels=128, fmin=20, fmax=16000, n_fft=1024, hop_length=256):\n",
    "    \"\"\"Compute mel-spectrogram with slightly denser time resolution (hop_length=256 instead of 512)\"\"\"\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=audio, \n",
    "        sr=sr, \n",
    "        n_mels=n_mels, \n",
    "        fmin=fmin, \n",
    "        fmax=fmax, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length, \n",
    "        power=2.0\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    \n",
    "    # Normalize to 0-1 range\n",
    "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-6)\n",
    "    return mel_norm\n",
    "\n",
    "def audio_to_melspec(audio, sr=32000, duration=5, augment=False):\n",
    "    \"\"\"Convert audio to mel-spectrogram with optional augmentations\"\"\"\n",
    "    target_len = int(sr * duration)\n",
    "    \n",
    "    # Handle audio length\n",
    "    if len(audio) > target_len:\n",
    "        start = np.random.randint(0, len(audio) - target_len)\n",
    "        audio = audio[start:start + target_len]\n",
    "    else:\n",
    "        # Pad if audio is shorter\n",
    "        pad = target_len - len(audio)\n",
    "        audio = np.pad(audio, (pad//2, pad - pad//2))\n",
    "    \n",
    "    # Apply augmentations if requested\n",
    "    if augment:\n",
    "        # Time shift\n",
    "        shift_factor = np.random.uniform(-0.1, 0.1)\n",
    "        shift = int(shift_factor * len(audio))\n",
    "        audio = np.roll(audio, shift)\n",
    "        \n",
    "        # Add random noise\n",
    "        noise_level = np.random.uniform(0, 0.01)\n",
    "        noise = np.random.normal(0, noise_level, len(audio))\n",
    "        audio = audio + noise\n",
    "        \n",
    "        # Random gain\n",
    "        gain = np.random.uniform(0.8, 1.2)\n",
    "        audio = audio * gain\n",
    "        audio = np.clip(audio, -1, 1)\n",
    "    \n",
    "    # Convert to mel spectrogram\n",
    "    melspec = compute_melspec(audio, sr=sr)\n",
    "    return melspec[np.newaxis, :, :]  # add channel dimension\n",
    "\n",
    "# Display a sample mel-spectrogram\n",
    "def plot_melspec(melspec, title=\"Mel-Spectrogram\"):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(\n",
    "        melspec[0], \n",
    "        sr=32000, \n",
    "        hop_length=256, \n",
    "        x_axis='time', \n",
    "        y_axis='mel', \n",
    "        fmin=20, \n",
    "        fmax=16000\n",
    "    )\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa77138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize a sample audio\n",
    "sample_row = train_df.iloc[0]\n",
    "sample_path = os.path.join(DATA_PATH, 'train_audio', sample_row.filename)\n",
    "print(f\"Loading sample: {sample_row.filename} (Species: {sample_row.primary_label})\")\n",
    "\n",
    "audio, sr = sf.read(sample_path)\n",
    "melspec = audio_to_melspec(audio, sr=sr)\n",
    "plot_melspec(melspec, f\"Mel-Spectrogram: {sample_row.primary_label}\")\n",
    "\n",
    "# Plot waveform\n",
    "plt.figure(figsize=(10, 2))\n",
    "librosa.display.waveshow(audio[:sr*5], sr=sr)\n",
    "plt.title(f\"Waveform: {sample_row.primary_label}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abe978",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "Define PyTorch dataset for the Audio Transformer with patch embedding preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d310d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransformerDataset(Dataset):\n",
    "    def __init__(self, df, path, sr=32000, duration=5, augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.path = path\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Create label mappings\n",
    "        self.labels = sorted(df['primary_label'].unique())\n",
    "        self.label2idx = {label: idx for idx, label in enumerate(self.labels)}\n",
    "        self.idx2label = {idx: label for idx, label in enumerate(self.labels)}\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.labels)} classes and {len(df)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = os.path.join(self.path, 'train_audio', row.filename)\n",
    "        \n",
    "        # Load audio\n",
    "        audio, _ = sf.read(audio_path)\n",
    "        \n",
    "        # Convert to mel spectrogram with augmentation\n",
    "        melspec = audio_to_melspec(audio, sr=self.sr, duration=self.duration, augment=self.augment)\n",
    "        \n",
    "        # Get label\n",
    "        label_idx = self.label2idx[row.primary_label]\n",
    "        \n",
    "        return torch.tensor(melspec, dtype=torch.float32), torch.tensor(label_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa21521",
   "metadata": {},
   "source": [
    "## Audio Transformer Model Definition\n",
    "\n",
    "Here we define the transformer-based architecture with self-attention mechanisms to capture long-range dependencies in audio data. The model follows a Vision Transformer (ViT) style but adapted for audio spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b97f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert mel-spectrogram into patches and embed them\"\"\"\n",
    "    def __init__(self, patch_size=(16, 16), emb_dim=192, in_channels=1, img_size=(128, 626)):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "        # Linear projection to embedding dimension\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, emb_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        x = self.proj(x)  # [B, E, H', W']\n",
    "        # Flatten patches into sequence\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, E]\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder with multi-head self-attention\"\"\"\n",
    "    def __init__(self, emb_dim=192, num_heads=8, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(emb_dim * mlp_ratio, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, seq_len, emb_dim]\n",
    "        attn_input = self.norm1(x)\n",
    "        attn_output, _ = self.attn(attn_input, attn_input, attn_input)\n",
    "        x = x + attn_output\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class AudioTransformer(nn.Module):\n",
    "    \"\"\"Audio Transformer for bioacoustic classification\"\"\"\n",
    "    def __init__(self, num_classes, emb_dim=192, depth=12, num_heads=8, \n",
    "                 mlp_ratio=4, dropout=0.1, img_size=(128, 626)):\n",
    "        super().__init__()\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=(16, 16),\n",
    "            emb_dim=emb_dim,\n",
    "            in_channels=1,\n",
    "            img_size=img_size\n",
    "        )\n",
    "        \n",
    "        # Class token and position embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches + 1, emb_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoder(emb_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final normalization and classification head\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.head = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights_)\n",
    "    \n",
    "    def _init_weights_(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        b = x.shape[0]\n",
    "        x = self.patch_embed(x)  # [B, num_patches, emb_dim]\n",
    "        \n",
    "        # Add class token\n",
    "        cls_token = self.cls_token.expand(b, -1, -1)  # [B, 1, emb_dim]\n",
    "        x = torch.cat([cls_token, x], dim=1)  # [B, num_patches + 1, emb_dim]\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Process through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Classification based on CLS token\n",
    "        x = self.norm(x)\n",
    "        x = self.head(x[:, 0])  # Use CLS token for classification\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a small model to check the architecture\n",
    "dummy_data = torch.randn(2, 1, 128, 626)  # Batch of 2, 1 channel, 128x626 spectrogram\n",
    "small_model = AudioTransformer(num_classes=10, emb_dim=64, depth=2, num_heads=2)\n",
    "output = small_model(dummy_data)\n",
    "print(f\"Input shape: {dummy_data.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in small_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eccb1c",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "Define training and validation loops with mixed precision for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "    \n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use mixed precision for faster training\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "        \n",
    "        # Scale gradients and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%\")\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "        \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "            \n",
    "            # Store outputs and targets for ROC-AUC calculation\n",
    "            all_outputs.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "            all_targets.append(F.one_hot(y, num_classes=outputs.size(1)).cpu().numpy())\n",
    "    \n",
    "    # Calculate AUC\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Only calculate AUC for classes that have positive examples\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    pos_classes = (all_targets.sum(0) > 0)\n",
    "    auc = roc_auc_score(all_targets[:, pos_classes], all_outputs[:, pos_classes], average='macro')\n",
    "    \n",
    "    return running_loss / total, correct / total, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c8a17",
   "metadata": {},
   "source": [
    "## Main Training Routine\n",
    "Prepare loaders, model, and train with learning rate scheduling and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_epochs=30, batch_size=16, lr=3e-5):\n",
    "    print(\"Starting training process...\")\n",
    "    \n",
    "    # Prepare data with stratified split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_df, \n",
    "        test_size=0.2, \n",
    "        stratify=train_df.primary_label, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AudioTransformerDataset(train_data, DATA_PATH, augment=True)\n",
    "    val_dataset = AudioTransformerDataset(val_data, DATA_PATH, augment=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size*2, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize the model\n",
    "    num_classes = len(train_dataset.labels)\n",
    "    model = AudioTransformer(\n",
    "        num_classes=num_classes,\n",
    "        emb_dim=192,       # Embedding dimension\n",
    "        depth=8,           # Number of transformer layers (reduced for training speed)\n",
    "        num_heads=8,       # Multi-head attention heads\n",
    "        mlp_ratio=4,       # MLP hidden dim ratio\n",
    "        dropout=0.2        # Dropout rate for regularization\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_epochs)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_auc = 0\n",
    "    patience = 5\n",
    "    wait = 0\n",
    "    \n",
    "    # Create directory for model checkpoints if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_aucs = []\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{train_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_auc = validate(model, val_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} results:\")\n",
    "        print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Valid - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save(model.state_dict(), 'models/audio_transformer_best.pt')\n",
    "            print(f\"  Improved! New best AUC: {best_auc:.4f}\")\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            print(f\"  No improvement for {wait} epochs.\")\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('models/audio_transformer_best.pt'))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.plot(val_aucs, label='Val AUC')\n",
    "    plt.title('Accuracy & AUC Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('audio_transformer_training.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training complete! Best validation AUC: {best_auc:.4f}\")\n",
    "    return model, train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6790b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (comment out during development if needed)\n",
    "model, train_dataset = main(train_epochs=30, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf497a6f",
   "metadata": {},
   "source": [
    "## Inference and Submission\n",
    "Implement sliding-window inference on test soundscapes with overlapping windows for more robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_audio(model, audio_path, sr=32000, window_size=5, step_size=2.5):\n",
    "    \"\"\"Run inference on a full audio file using sliding window approach\"\"\"\n",
    "    # Load audio\n",
    "    audio, _ = sf.read(audio_path)\n",
    "    \n",
    "    # Initialize predictions list\n",
    "    all_preds = []\n",
    "    \n",
    "    # Calculate window and step sizes in samples\n",
    "    window_samples = int(window_size * sr)\n",
    "    step_samples = int(step_size * sr)\n",
    "    \n",
    "    # Create sliding windows\n",
    "    for start in range(0, max(1, len(audio) - window_samples + 1), step_samples):\n",
    "        end = min(start + window_samples, len(audio))\n",
    "        window = audio[start:end]\n",
    "        \n",
    "        # If window is too short, pad it\n",
    "        if len(window) < window_samples:\n",
    "            pad = window_samples - len(window)\n",
    "            window = np.pad(window, (0, pad))\n",
    "        \n",
    "        # Convert to mel spectrogram and make prediction\n",
    "        melspec = audio_to_melspec(window, sr=sr)\n",
    "        melspec_tensor = torch.tensor(melspec, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(melspec_tensor)\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "            \n",
    "        all_preds.append(probs)\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_preds = np.mean(all_preds, axis=0)\n",
    "    \n",
    "    return avg_preds\n",
    "\n",
    "def generate_submission():\n",
    "    \"\"\"Generate submission file for Kaggle\"\"\"\n",
    "    print(\"Generating submission file...\")\n",
    "    \n",
    "    # Load the best model\n",
    "    num_classes = len(train_dataset.labels)\n",
    "    model = AudioTransformer(\n",
    "        num_classes=num_classes,\n",
    "        emb_dim=192, \n",
    "        depth=8, \n",
    "        num_heads=8,\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.0  # No dropout for inference\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load('models/audio_transformer_best.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load sample submission to get format\n",
    "    sample_submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n",
    "    \n",
    "    # Extract unique soundscape IDs from row_id\n",
    "    soundscape_ids = set()\n",
    "    for row_id in sample_submission['row_id']:\n",
    "        parts = row_id.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            soundscape_ids.add(parts[1])\n",
    "    \n",
    "    # Create dictionary to store predictions for each soundscape\n",
    "    soundscape_preds = {}\n",
    "    \n",
    "    # Run inference on each soundscape\n",
    "    for sid in tqdm(soundscape_ids):\n",
    "        audio_path = os.path.join(DATA_PATH, 'test_soundscapes', f'soundscape_{sid}.ogg')\n",
    "        if os.path.exists(audio_path):\n",
    "            preds = predict_on_audio(model, audio_path)\n",
    "            soundscape_preds[sid] = preds\n",
    "        else:\n",
    "            print(f\"Warning: File not found - {audio_path}\")\n",
    "    \n",
    "    # Map predictions to row_ids in the submission format\n",
    "    results = []\n",
    "    for _, row in sample_submission.iterrows():\n",
    "        row_id = row['row_id']\n",
    "        parts = row_id.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            sid = parts[1]\n",
    "            if sid in soundscape_preds:\n",
    "                preds = soundscape_preds[sid]\n",
    "                results.append([row_id] + list(preds))\n",
    "            else:\n",
    "                # If we missed this soundscape, use zeros\n",
    "                results.append([row_id] + [0] * num_classes)\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission = pd.DataFrame(results, columns=sample_submission.columns)\n",
    "    \n",
    "    # Save submission file\n",
    "    submission.to_csv('audio_transformer_submission.csv', index=False)\n",
    "    print(\"Submission file saved to 'audio_transformer_submission.csv'\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa880c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file\n",
    "submission = generate_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da0f1e",
   "metadata": {},
   "source": [
    "## Ensemble Integration\n",
    "Export model weights and predictions for later ensemble integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce73b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_for_ensemble(model, export_path='ensemble_exports', model_name='audio_transformer'):\n",
    "    \"\"\"Save model weights and OOF predictions for ensemble integration\"\"\"\n",
    "    # Create directory\n",
    "    os.makedirs(export_path, exist_ok=True)\n",
    "    \n",
    "    # Save model in multiple formats\n",
    "    torch.save(model.state_dict(), f'{export_path}/{model_name}.pt')\n",
    "    \n",
    "    # Export to ONNX (for faster inference)\n",
    "    dummy_input = torch.randn(1, 1, 128, 626).to(DEVICE)\n",
    "    torch.onnx.export(\n",
    "        model, \n",
    "        dummy_input, \n",
    "        f\"{export_path}/{model_name}.onnx\",\n",
    "        export_params=True,\n",
    "        opset_version=12,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exported to {export_path}/{model_name}.pt and {model_name}.onnx\")\n",
    "    print(\"These files can now be used in the ensemble integration notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for ensemble integration\n",
    "save_for_ensemble(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638c6a0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implements an Audio Transformer model for the BirdCLEF+ 2025 competition. The transformer architecture leverages self-attention mechanisms to capture long-range dependencies in audio data, which is particularly useful for identifying complex patterns like overlapping calls from multiple species.\n",
    "\n",
    "Key aspects of this implementation:\n",
    "\n",
    "1. **Transformer-based architecture**: Adapts Vision Transformer principles to audio spectrograms\n",
    "2. **Patch embedding**: Splits spectrograms into patches for transformer processing\n",
    "3. **Mixed precision training**: Uses FP16 for faster training\n",
    "4. **Sliding window inference**: Processes longer audio files with overlapping windows\n",
    "5. **Export to ensemble**: Ready for integration with other models in the ensemble\n",
    "\n",
    "This model provides a different inductive bias compared to the CNN and CRNN models, enriching the ensemble approach by capturing global self-attention patterns in the audio data. The next step is to integrate this model with the others in the ensemble."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
