{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edb238a",
   "metadata": {},
   "source": [
    "# BirdCLEF+ 2025: CRNN with Precomputed Mel Spectrograms\n",
    "This notebook implements a Convolutional Recurrent Neural Network (CRNN) for the BirdCLEF+ 2025 competition using precomputed mel spectrograms. This approach provides consistency with the CNN model and faster training by reusing the same features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe60522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "\n",
    "# Define paths for Colab\n",
    "DATA_PATH = '/content/drive/MyDrive/birdclef-2025-data'\n",
    "MODEL_SAVE_DIR = '/content/drive/MyDrive/birdclef-2025-models'\n",
    "PRECOMPUTED_FEATURES_PATH = '/content/drive/MyDrive/bird_features_scratch_copy.npz'  # Path to precomputed features\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q librosa scikit-learn\n",
    "\n",
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility and device setup\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14234bf8",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "We'll load the precomputed mel spectrogram features from .npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e620bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precomputed_features(local_path=PRECOMPUTED_FEATURES_PATH):\n",
    "    \"\"\"Load precomputed features from saved file.\"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"ERROR: Precomputed features file not found at {local_path}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load the features\n",
    "    print(f\"Loading precomputed features from {local_path}...\")\n",
    "    try:\n",
    "        data = np.load(local_path)\n",
    "        X = data['data']\n",
    "        y = data['labels']\n",
    "        print(f\"Successfully loaded precomputed features!\")\n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "        print(f\"Number of unique classes: {len(np.unique(y))}\")\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading precomputed features: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load the precomputed features\n",
    "X, y = load_precomputed_features()\n",
    "\n",
    "# Handle classes with only one sample (if any)\n",
    "if X is not None and y is not None:\n",
    "    label_counts = np.bincount(y)\n",
    "    print(f\"Number of classes with only one sample: {np.sum(label_counts == 1)}\")\n",
    "    \n",
    "    if np.any(label_counts == 1):\n",
    "        print(\"Handling classes with only one sample...\")\n",
    "        # Remove classes with only one sample\n",
    "        valid_indices = np.isin(y, np.where(label_counts >= 2)[0])\n",
    "        X = X[valid_indices]\n",
    "        y = y[valid_indices]\n",
    "        print(f\"Removed {np.sum(~valid_indices)} samples with singleton classes\")\n",
    "    \n",
    "    print(f\"Final feature shape: {X.shape}\")\n",
    "    print(f\"Number of unique classes after filtering: {len(np.unique(y))}\")\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6d57f",
   "metadata": {},
   "source": [
    "## Dataset for Precomputed Features\n",
    "Define a PyTorch dataset to work with our precomputed mel spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedFeatureDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for precomputed mel spectrogram features\"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Add channel dimension if not already present\n",
    "        feature = self.features[idx]\n",
    "        if len(feature.shape) == 2:\n",
    "            feature = feature[np.newaxis, :, :]\n",
    "        return torch.tensor(feature, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0389bf",
   "metadata": {},
   "source": [
    "## CRNN Model for Precomputed Features\n",
    "Define our CRNN architecture to work with precomputed mel spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db614850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedFeatureCRNN(nn.Module):\n",
    "    \"\"\"CRNN model that works directly with precomputed mel spectrogram features\"\"\"\n",
    "    def __init__(self, num_classes, gru_hidden_size=256):\n",
    "        super(PrecomputedFeatureCRNN, self).__init__()\n",
    "        \n",
    "        # Input feature shape for precomputed melspectrogram (height, width, channels)\n",
    "        # Assuming shape is [1, 128, 256] (channels, height, width)\n",
    "        input_channels = 1\n",
    "        \n",
    "        # CNN feature extractor (simplified from BirdCNN)\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "        \n",
    "        # Calculate feature dimensions after CNN\n",
    "        # After 3 pool layers: 128/(2^3) x 256/(2^3) = 16 x 32\n",
    "        # Feature channels: 256\n",
    "        self.feature_dim = 256\n",
    "        \n",
    "        # GRU takes features from each time column (treating width as time steps)\n",
    "        # Input size is height * channels after the last CNN layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=16 * 256,  # Height * channels after CNN\n",
    "            hidden_size=gru_hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size*2, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size*2, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x shape: [batch, channels, height, width]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Extract features with CNN\n",
    "        features = self.features(x)  # Shape: [batch, 256, 16, 32]\n",
    "        \n",
    "        # Prepare features for GRU\n",
    "        # We'll treat width as time steps (32 time steps)\n",
    "        # For each time step, we have a feature vector of size [channels * height]\n",
    "        \n",
    "        # Reshape to [batch, width(time), channels * height]\n",
    "        features = features.permute(0, 3, 1, 2)  # [batch, 32, 256, 16]\n",
    "        features = features.reshape(batch_size, features.size(1), -1)  # [batch, 32, 256*16]\n",
    "        \n",
    "        # Process with GRU\n",
    "        gru_out, _ = self.gru(features)  # [batch, 32, 2*hidden_size]\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_weights = self.attention(gru_out).squeeze(-1)  # [batch, 32]\n",
    "        attn_weights = F.softmax(attn_weights, dim=1).unsqueeze(1)  # [batch, 1, 32]\n",
    "        \n",
    "        # Compute weighted sum\n",
    "        context = torch.bmm(attn_weights, gru_out).squeeze(1)  # [batch, 2*hidden_size]\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(context)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ea4b5",
   "metadata": {},
   "source": [
    "## Visualize Training Results Function\n",
    "Define a function to visualize training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation AUC\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['val_auc'], label='Validation')\n",
    "    plt.title('ROC AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33eda6",
   "metadata": {},
   "source": [
    "## Training Function for Precomputed Features\n",
    "Define the training and validation procedure for our CRNN model with precomputed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_precomputed_crnn(y):\n",
    "    \"\"\"Train CRNN model using precomputed features from .npz file\"\"\"\n",
    "    if X_train is None or y_train is None:\n",
    "        print(\"Precomputed features not loaded properly. Please check the path.\")\n",
    "        return None, None, None\n",
    "        \n",
    "    # Create datasets from precomputed features\n",
    "    train_dataset = PrecomputedFeatureDataset(X_train, y_train)\n",
    "    val_dataset = PrecomputedFeatureDataset(X_val, y_val)\n",
    "    \n",
    "    # Create data loaders with optimized batch sizes for A100\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=512,  # Larger batch size possible with precomputed features\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Number of classes from the data\n",
    "    num_classes = len(np.unique(y))\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Initialize the PrecomputedFeatureCRNN model\n",
    "    model = PrecomputedFeatureCRNN(\n",
    "        num_classes=num_classes,\n",
    "        gru_hidden_size=256\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Set up loss function, optimizer, and scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer with appropriate learning rates\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.features.parameters(), 'lr': 3e-4},\n",
    "        {'params': model.gru.parameters(), 'lr': 3e-4},\n",
    "        {'params': model.attention.parameters(), 'lr': 3e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 3e-4}\n",
    "    ], weight_decay=0.01)\n",
    "    \n",
    "    # Learning rate scheduler based on validation AUC\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Set up mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs = 50\n",
    "    best_auc = 0\n",
    "    patience = 7\n",
    "    wait = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "    model_name = \"crnn_precomputed\"\n",
    "    best_model_path = os.path.join(MODEL_SAVE_DIR, f\"{model_name}_best.pt\")\n",
    "    \n",
    "    # Training loop with mixed precision\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "            \n",
    "            # Backward and optimize with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_outputs = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with autocast():\n",
    "                    outputs = model(x)\n",
    "                    loss = criterion(outputs, y)\n",
    "                \n",
    "                # Update statistics\n",
    "                val_running_loss += loss.item() * x.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += y.size(0)\n",
    "                val_correct += predicted.eq(y).sum().item()\n",
    "                \n",
    "                # Store outputs and targets for AUC calculation\n",
    "                all_outputs.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "                all_targets.append(F.one_hot(y, num_classes=outputs.size(1)).cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update history\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Calculate ROC-AUC\n",
    "        try:\n",
    "            all_outputs = np.concatenate(all_outputs)\n",
    "            all_targets = np.concatenate(all_targets)\n",
    "            \n",
    "            # Classes with positive examples\n",
    "            pos = (all_targets.sum(0) > 0)\n",
    "            val_auc = roc_auc_score(all_targets[:, pos], all_outputs[:, pos], average='macro')\n",
    "            history['val_auc'].append(val_auc)\n",
    "            \n",
    "            # Print results\n",
    "            print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, '\n",
    "                  f'val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_auc={val_auc:.4f}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating AUC: {e}\")\n",
    "            val_auc = history['val_auc'][-1] if history['val_auc'] else 0\n",
    "            history['val_auc'].append(val_auc)\n",
    "        \n",
    "        # Update scheduler based on validation AUC\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_auc': val_auc,\n",
    "            }, best_model_path)\n",
    "            wait = 0\n",
    "            print(f\"New best model with val_auc={val_auc:.4f}\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Training complete. Best validation AUC: {best_auc:.4f} at epoch {checkpoint['epoch']+1}\")\n",
    "    \n",
    "    # Save final model in a format suitable for ensemble\n",
    "    label_mapping = {i: f\"class_{i}\" for i in range(num_classes)}\n",
    "    torch.save({\n",
    "        'model_state': model.state_dict(),\n",
    "        'model_type': 'crnn_precomputed',\n",
    "        'input_params': {\n",
    "            'sr': 32000,\n",
    "            'n_mels': 128,\n",
    "            'fmin': 20,\n",
    "            'fmax': 16000,\n",
    "            'n_fft': 1024,\n",
    "            'hop_length': 512,\n",
    "            'duration': 5\n",
    "        },\n",
    "        'history': history\n",
    "    }, os.path.join(MODEL_SAVE_DIR, f\"{model_name}_ensemble.pth\"))\n",
    "    \n",
    "    print(f\"Model saved for ensemble use at '{os.path.join(MODEL_SAVE_DIR, f'{model_name}_ensemble.pth')}'\")\n",
    "    \n",
    "    return model, label_mapping, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89f512",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "Execute the training process with precomputed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CRNN using precomputed features from the .npz file\n",
    "precomputed_model, precomputed_label_mapping, precomputed_history = train_precomputed_crnn(y)\n",
    "\n",
    "# Plot the training history\n",
    "if precomputed_history is not None:\n",
    "    plot_history(precomputed_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a395a",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the trained model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_precomputed_model(model):\n",
    "    val_dataset = PrecomputedFeatureDataset(X_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=256, \n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(val_loader):\n",
    "            x = x.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            preds = outputs.argmax(1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y.numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Compute and plot confusion matrix for top classes\n",
    "    class_counts = np.bincount(y_val)\n",
    "    top_classes = np.argsort(class_counts)[-20:]  # Top 20 most frequent classes\n",
    "    \n",
    "    # Filter predictions and labels for top classes only\n",
    "    mask = np.isin(all_labels, top_classes)\n",
    "    filtered_preds = [all_preds[i] for i, m in enumerate(mask) if m]\n",
    "    filtered_labels = [all_labels[i] for i, m in enumerate(mask) if m]\n",
    "    \n",
    "    # Compute and plot confusion matrix\n",
    "    cm = confusion_matrix(filtered_labels, filtered_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=[f\"Class {i}\" for i in top_classes],\n",
    "              yticklabels=[f\"Class {i}\" for i in top_classes])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Top 20 Classes)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Display top and bottom performing classes\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    top_performing = df_report.sort_values(by='f1-score', ascending=False).head(10)\n",
    "    bottom_performing = df_report.sort_values(by='f1-score').head(10)\n",
    "    \n",
    "    print(\"Top 10 best predicted classes:\")\n",
    "    print(top_performing[['precision', 'recall', 'f1-score', 'support']])\n",
    "    \n",
    "    print(\"\\nBottom 10 worst predicted classes:\")\n",
    "    print(bottom_performing[['precision', 'recall', 'f1-score', 'support']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "if precomputed_model is not None:\n",
    "    evaluate_precomputed_model(precomputed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd2536",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully implemented a CRNN model for the BirdCLEF+ 2025 competition using precomputed mel spectrograms. This approach provides several advantages:\n",
    "\n",
    "1. **Consistency with CNN model**: Using the same precomputed features ensures both models see the same input representation\n",
    "2. **Faster training**: No need to compute mel spectrograms on-the-fly\n",
    "3. **Efficiency**: Allows for larger batch sizes and faster iteration\n",
    "4. **Ensemble compatibility**: Making it easier to combine model predictions later\n",
    "\n",
    "The CRNN model combines convolutional layers to extract spatial features from mel spectrograms with recurrent layers that capture temporal dynamics, enhanced with an attention mechanism. This architecture is well-suited for capturing both the frequency patterns and temporal evolution of bird calls, which is essential for accurate species classification.\n",
    "\n",
    "This model now forms a key component of our ensemble approach for the BirdCLEF+ 2025 competition."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
